{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa75dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\\n",
    "from qdrant_client.models import PointStruct, Distance, VectorParams, ScoredPoint, Filter, FieldCondition, MatchValue\\n",
    "from sentence_transformers import SentenceTransformer, util\\n",
    "import pandas as pd\\n",
    "import hashlib\\n",
    "from datetime import datetime, timezone\\n",
    "import os\\n",
    "import uuid\\n",
    "import numpy as np\\n",
    "from bson import ObjectId\\n",
    "import torch\\n",
    "import numpy as np\\n",
    "from scipy.stats import pearsonr\\n",
    "from scipy.stats import spearmanr\\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view_text_website_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_text_website(url):\\n",
    "    \\"\\"\\"\\n",
    "    Fetches the content of a website as plain text.\\n",
    "    \\"\\"\\"\\n",
    "    try:\\n",
    "        response = requests.get(url, timeout=10)\\n",
    "        response.raise_for_status()\\n",
    "        return response.text\\n",
    "    except requests.RequestException as e:\\n",
    "        print(f\\"Error fetching {url}: {e}\\")\\n",
    "        return \\"\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe83496",
   "metadata": {},
   "source": [
    "#### Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\\n",
    "\\n",
    "mongo_uri = \\"mongodb://localhost:27017/\\"\\n",
    "mongo_client = MongoClient(mongo_uri)\\n",
    "db = mongo_client['pipelineos']\\n",
    "\\n",
    "icp_profiles = db[\\"icp_profiles\\"]\\n",
    "discovered_companies = db[\\"discovered_companies\\"]\\n",
    "scored_companies = db['scored_companies']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0b9f3",
   "metadata": {},
   "source": [
    "#### Qdrant connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import VectorParams, Distance\\n",
    "qdrant = QdrantClient(host=\\"localhost\\", port=6333)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25c3ef",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b1c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\\"all-MiniLM-L12-v2\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81305cee",
   "metadata": {},
   "source": [
    "#### To fetch the latest ICP from the MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcf1e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\\n",
    "    {\\"$match\\": {\\"active\\": True}},\\n",
    "    {\\"$addFields\\": {\\"effective_time\\": {\\"$cond\\": {\\"if\\": {\\"$gt\\": [\\"$updated_at\\", \\"$created_at\\"]}, \\"then\\": \\"$updated_at\\", \\"else\\": \\"$created_at\\"}}}},\\n",
    "    {\\"$sort\\": {\\"effective_time\\": -1}},\\n",
    "    {\\"$limit\\": 1}\\n",
    "]\\n",
    "latest_icp = list(icp_profiles.aggregate(pipeline))\\n",
    "if latest_icp:\\n",
    "    latest_icp = latest_icp[0]\\n",
    "    print(\\"Latest active ICP:\\")\\n",
    "    print(latest_icp)\\n",
    "else:\\n",
    "    print(\\"No active ICP found.\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a293b3f7_markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weights_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS = {\\n",
    "    \\"industry\\": 20,\\n",
    "    \\"employee_count\\": 15,\\n",
    "    \\"location\\": 15,\\n",
    "    \\"tech_stack\\": 25,\\n",
    "    \\"keywords\\": 10,\\n",
    "    \\"founded_year\\": 5,\\n",
    "    \\"github_signal\\": 10\\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rule_score_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_score(company_doc, icp_field_tokens):\\n",
    "    rule_score_val = 0\\n",
    "    breakdown = {k: 0 for k in WEIGHTS}\\n",
    "    def safe_list(field):\\n",
    "        value = company_doc.get(field, [])\\n",
    "        if isinstance(value, str): return [value]\\n",
    "        if isinstance(value, list): return value\\n",
    "        return []\\n",
    "    if 'industry' in icp_field_tokens:\\n",
    "        company_industries_text = \\" \\".join(safe_list(\\"industries\\")).lower()\\n",
    "        company_industry_words = set(company_industries_text.split())\\n",
    "        if not company_industry_words.isdisjoint(icp_field_tokens['industry']):\\n",
    "            rule_score_val += WEIGHTS[\\"industry\\"]\\n",
    "            breakdown[\\"industry\\"] = WEIGHTS[\\"industry\\"]\\n",
    "    if 'location' in icp_field_tokens:\\n",
    "        company_location_parts = []\\n",
    "        hq_location = company_doc.get(\\"hq_location\\")\\n",
    "        if isinstance(hq_location, str):\\n",
    "            company_location_parts.append(hq_location)\\n",
    "        elif isinstance(hq_location, list):\\n",
    "            company_location_parts.extend(hq_location)\\n",
    "        elif isinstance(hq_location, dict):\\n",
    "            company_location_parts.extend(hq_location.values())\\n",
    "        company_location_text = \\" \\".join(str(p) for p in company_location_parts).lower()\\n",
    "        company_location_words = set(company_location_text.split())\\n",
    "        if not company_location_words.isdisjoint(icp_field_tokens['location']):\\n",
    "            rule_score_val += WEIGHTS[\\"location\\"]\\n",
    "            breakdown[\\"location\\"] = WEIGHTS[\\"location\\"]\\n",
    "    if 'tech_stack' in icp_field_tokens:\\n",
    "        company_tech = {v.lower() for v in safe_list(\\"tech_stack\\")}\\n",
    "        if not company_tech.isdisjoint(icp_field_tokens['tech_stack']):\\n",
    "            rule_score_val += WEIGHTS[\\"tech_stack\\"]\\n",
    "            breakdown[\\"tech_stack\\"] = WEIGHTS[\\"tech_stack\\"]\\n",
    "    if 'keywords' in icp_field_tokens:\\n",
    "        text_to_search = \\" \\".join([company_doc.get(\\"name\\", \\"\\"), \\" \\".join(safe_list(\\"industries\\")), \\" \\".join(safe_list(\\"tech_stack\\"))]).lower()\\n",
    "        if any(token in text_to_search for token in icp_field_tokens['keywords']):\\n",
    "            rule_score_val += WEIGHTS[\\"keywords\\"]\\n",
    "            breakdown[\\"keywords\\"] = WEIGHTS[\\"keywords\\"]\\n",
    "    if 'employee_count' in icp_field_tokens:\\n",
    "        emp = company_doc.get(\\"employee_count_estimate\\", {})\\n",
    "        emp_min = emp.get(\\"min\\", 0)\\n",
    "        emp_max = emp.get(\\"max\\", float('inf'))\\n",
    "        icp_emp = icp_field_tokens['employee_count']\\n",
    "        icp_min = icp_emp.get('min')\\n",
    "        icp_max = icp_emp.get('max')\\n",
    "        if icp_min is not None and icp_max is not None:\\n",
    "            if not (emp_max < icp_min or emp_min > icp_max):\\n",
    "                rule_score_val += WEIGHTS[\\"employee_count\\"]\\n",
    "                breakdown[\\"employee_count\\"] = WEIGHTS[\\"employee_count\\"]\\n",
    "    if 'founded_after' in icp_field_tokens:\\n",
    "        founded_year = company_doc.get(\\"founded_year\\")\\n",
    "        if isinstance(founded_year, int) and founded_year >= icp_field_tokens['founded_after']:\\n",
    "            rule_score_val += WEIGHTS[\\"founded_year\\"]\\n",
    "            breakdown[\\"founded_year\\"] = WEIGHTS[\\"founded_year\\"]\\n",
    "    github_urls = [url for url in safe_list(\\"source_urls\\") if \\"github.com\\" in url.lower()]\\n",
    "    if github_urls:\\n",
    "        icp_tech_stack = icp_field_tokens.get('tech_stack', set())\\n",
    "        if icp_tech_stack:\\n",
    "            github_tech_stack = set()\\n",
    "            for url in github_urls:\\n",
    "                try:\\n",
    "                    page_content = view_text_website(url)\\n",
    "                    for tech in icp_tech_stack:\\n",
    "                        if tech.lower() in page_content.lower():\\n",
    "                            github_tech_stack.add(tech.lower())\\n",
    "                    if \\"requirements.txt\\" in page_content:\\n",
    "                        github_tech_stack.add(\\"python\\")\\n",
    "                    if \\"package.json\\" in page_content:\\n",
    "                        github_tech_stack.add(\\"javascript\\")\\n",
    "                        github_tech_stack.add(\\"typescript\\")\\n",
    "                    if \\"pom.xml\\" in page_content or \\"build.gradle\\" in page_content:\\n",
    "                        github_tech_stack.add(\\"java\\")\\n",
    "                        github_tech_stack.add(\\"kotlin\\")\\n",
    "                except Exception as e:\\n",
    "                    print(f\\"Could not process GitHub URL {url}: {e}\\")\\n",
    "            if not github_tech_stack.isdisjoint(icp_tech_stack):\\n",
    "                rule_score_val += WEIGHTS[\\"github_signal\\"]\\n",
    "                breakdown[\\"github_signal\\"] = WEIGHTS[\\"github_signal\\"]\\n",
    "    return rule_score_val, breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main_scoring_loop_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "if latest_icp:\\n",
    "    icp = latest_icp\\n",
    "    icp_id = str(icp[\\"_id\\"])\\n",
    "    icp_version = icp.get(\\"version\\", 1)\\n",
    "\\n",
    "    icp_vector_resp = qdrant.retrieve(\\n",
    "        collection_name=\\"icp_vectors\\",\\n",
    "        ids=[str(uuid.uuid5(uuid.NAMESPACE_DNS, f\\"icp:{icp_id}\\"))],\\n",
    "        with_vectors=True\\n",
    "    )\\n",
    "    if not icp_vector_resp or not icp_vector_resp[0].vector:\\n",
    "        print(\\"❌ ICP vector not found.\\")\\n",
    "    else:\\n",
    "        icp_embedding = icp_vector_resp[0].vector\\n",
    "        search_result = qdrant.query_points(\\n",
    "            collection_name=\\"company_vectors\\",\\n",
    "            query=icp_embedding,\\n",
    "            limit=10000,\\n",
    "            with_payload=True,\\n",
    "            with_vectors=True\\n",
    "        )\\n",
    "        print(f\\"Total points returned from Qdrant: {len(search_result.points)}\\")\\n",
    "\\n",
    "        icp_filters = icp.get('filters', {})\\n",
    "        icp_field_tokens = {}\\n",
    "        all_tokens = []\\n",
    "        industry_tokens = []\\n",
    "        for field_name in ['industry', 'industries']:\\n",
    "            if field_name in icp_filters:\\n",
    "                industries = icp_filters[field_name]\\n",
    "                if isinstance(industries, list):\\n",
    "                    for industry in industries:\\n",
    "                        industry_tokens.extend(str(industry).lower().split())\\n",
    "                elif isinstance(industries, str):\\n",
    "                    industry_tokens.extend(industries.lower().split())\\n",
    "        if industry_tokens:\\n",
    "            icp_field_tokens['industry'] = set(industry_tokens)\\n",
    "            all_tokens.extend(industry_tokens)\\n",
    "        tech_tokens = []\\n",
    "        if 'tech_stack' in icp_filters:\\n",
    "            tech_stack = icp_filters['tech_stack']\\n",
    "            if isinstance(tech_stack, list):\\n",
    "                for tech in tech_stack:\\n",
    "                    tech_tokens.append(str(tech).lower())\\n",
    "            elif isinstance(tech_stack, str):\\n",
    "                tech_tokens.append(tech_stack.lower())\\n",
    "        if tech_tokens:\\n",
    "            icp_field_tokens['tech_stack'] = set(tech_tokens)\\n",
    "            all_tokens.extend(tech_tokens)\\n",
    "        keyword_tokens = []\\n",
    "        if 'keywords' in icp_filters:\\n",
    "            keywords = icp_filters['keywords']\\n",
    "            if isinstance(keywords, list):\\n",
    "                for keyword in keywords:\\n",
    "                    keyword_tokens.append(str(keyword).lower())\\n",
    "            elif isinstance(keywords, str):\\n",
    "                keyword_tokens.append(keywords.lower())\\n",
    "        if keyword_tokens:\\n",
    "            icp_field_tokens['keywords'] = set(keyword_tokens)\\n",
    "            all_tokens.extend(keyword_tokens)\\n",
    "        location_tokens = []\\n",
    "        for field_name in ['location', 'locations']:\\n",
    "            if field_name in icp_filters:\\n",
    "                locations = icp_filters[field_name]\\n",
    "                if isinstance(locations, list):\\n",
    "                    for location in locations:\\n",
    "                        location_tokens.extend(str(location).lower().split())\\n",
    "                elif isinstance(locations, str):\\n",
    "                    location_tokens.extend(locations.lower().split())\\n",
    "        if location_tokens:\\n",
    "            icp_field_tokens['location'] = set(location_tokens)\\n",
    "            all_tokens.extend(location_tokens)\\n",
    "        if 'employee_count' in icp_filters:\\n",
    "            icp_field_tokens['employee_count'] = icp_filters['employee_count']\\n",
    "        if 'founded_after' in icp_filters:\\n",
    "            icp_field_tokens['founded_after'] = icp_filters['founded_after']\\n",
    "        icp_tokens = list(set(all_tokens))\\n",
    "        print(f\\"✅ Extracted ICP tokens: {icp_tokens}\\")\\n",
    "        print(f\\"✅ Field-specific tokens: {icp_field_tokens}\\")\\n",
    "\\n",
    "        scored_results_util = []\\n",
    "        for point in search_result.points:\\n",
    "            payload = point.payload\\n",
    "            company_id = payload.get(\\"company_id\\")\\n",
    "            company_vector = point.vector\\n",
    "            company_tensor = torch.tensor([company_vector], dtype=torch.float32)\\n",
    "            raw_similarity = util.cos_sim(torch.tensor([icp_embedding], dtype=torch.float32), company_tensor)[0][0].item()\\n",
    "            vector_similarity_score = ((raw_similarity + 1) / 2) * 100\\n",
    "            try:\\n",
    "                if isinstance(company_id, str):\\n",
    "                    company_id = ObjectId(company_id)\\n",
    "            except Exception as e:\\n",
    "                print(f'{company_id} has invalid ObjectId: {e}')\\n",
    "                continue\\n",
    "            company = discovered_companies.find_one({\\"_id\\": company_id})\\n",
    "            if not company:\\n",
    "                continue\\n",
    "            rule_score_total, breakdown = rule_score(company, icp_field_tokens)\\n",
    "            vector_weight = 0.4\\n",
    "            rule_weight = 0.6\\n",
    "            final_score = round((vector_similarity_score * vector_weight) + (rule_score_total * rule_weight), 2)\\n",
    "            breakdown[\\"vector_similarity\\"] = round(vector_similarity_score, 2)\\n",
    "            scored_doc = {\\"company_id\\": company_id, \\"icp_id\\": icp_id, \\"icp_version\\": icp_version, \\"final_score\\": final_score, \\"breakdown\\": breakdown, \\"weights\\": WEIGHTS, \\"last_scored\\": datetime.now(timezone.utc), \\"method\\": \\"util_cos_sim\\"}\\n",
    "            scored_companies.update_one({\\"company_id\\": company_id, \\"icp_id\\": icp_id, \\"method\\": \\"util_cos_sim\\"}, {\\"$set\\": scored_doc}, upsert=True)\\n",
    "            scored_results_util.append((company.get(\\"domain\\", \\"unknown\\"), final_score))\\n",
    "\\n",
    "        print(f\\"📈 Total companies scored: {len(scored_results_util)}\\")\\n",
    "        sorted_results = sorted(scored_results_util, key=lambda x: x[1], reverse=True)\\n",
    "        print(\\"\\n🏆 TOP RESULTS (Weighted Average Method):\\")\\n",
    "        for i, (domain, score) in enumerate(sorted_results[:10], 1):\\n",
    "            print(f\\"{i:2d}. {domain:20s} → {score:6.2f}%\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
