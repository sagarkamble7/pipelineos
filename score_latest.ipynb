{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa75dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagar/pipeline OS/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, Distance, VectorParams, ScoredPoint, Filter, FieldCondition, MatchValue\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "from bson import ObjectId\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe83496",
   "metadata": {},
   "source": [
    "#### Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "642a1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# mongo conection\n",
    "\n",
    "mongo_uri = \"mongodb://localhost:27017/\" #compass\n",
    "mongo_client = MongoClient(mongo_uri)\n",
    "db = mongo_client['pipelineos']\n",
    "\n",
    "# loading the mongodb into variables:\n",
    "\n",
    "icp_profiles = db[\"icp_profiles\"]\n",
    "discovered_companies = db[\"discovered_companies\"]\n",
    "scored_companies = db['scored_companies']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0b9f3",
   "metadata": {},
   "source": [
    "#### Qdrant connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69b0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qdrant connection via https\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "qdrant = QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333)\n",
    "#using qdrants https method as used 6333 port and gRPC is :\n",
    "# qdrant = QdrantClient(\n",
    "#     host=\"localhost\", \n",
    "#     port=6334, \n",
    "#     grpc=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017c1bb",
   "metadata": {},
   "source": [
    "#### Run the Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc600910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quadrant setup via docker:\n",
    "# run this in ubuntu terminal:\n",
    "\n",
    "# docker pull qdrant/qdrant\n",
    "# docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25c3ef",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "214b1c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vectorize setup:\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') #384-dim vector\n",
    "# model = SentenceTransformer(\"BAAI/bge-small-en\")  # 384 D\n",
    "model = SentenceTransformer(\"all-MiniLM-L12-v2\") # 384 D\n",
    "# model = SentenceTransformer(\"all-mpnet-base-v2\") # 768 d\n",
    "# model = SentenceTransformer(\"all-distilroberta-v1\") # 768\n",
    "# model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\") # 384 d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81305cee",
   "metadata": {},
   "source": [
    "#### To fetch the latest ICP from the MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fcf1e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest active ICP:\n",
      "{'_id': ObjectId('6899cb3aafa62213309e0357'), 'name': 'google', 'filters': {'locations': ['usa', 'canada'], 'tech_stack': ['AWS', 'python'], 'keywords': ['ai, ml'], 'designations': ['hr'], 'department': ['tect', 'aq', 'it'], 'industry': ['software', 'ai'], 'employee_count': {'min': 1, 'max': 500}, 'founded_after': 2019}, 'tags': ['tech', 'software'], 'active': True, 'created_at': datetime.datetime(2025, 8, 11, 10, 51, 38, 658000), 'updated_at': datetime.datetime(2025, 8, 11, 10, 51, 38, 658000), 'effective_time': datetime.datetime(2025, 8, 11, 10, 51, 38, 658000)}\n"
     ]
    }
   ],
   "source": [
    "# Aggregation pipeline to get the latest active ICP\n",
    "pipeline = [\n",
    "    {\"$match\": {\"active\": True}},\n",
    "    {\"$addFields\": {\n",
    "        \"effective_time\": {\n",
    "            \"$cond\": {\n",
    "                \"if\": {\"$gt\": [\"$updated_at\", \"$created_at\"]},\n",
    "                \"then\": \"$updated_at\",\n",
    "                \"else\": \"$created_at\"\n",
    "            }\n",
    "        }\n",
    "    }},\n",
    "    {\"$sort\": {\"effective_time\": -1}},\n",
    "    {\"$limit\": 1}\n",
    "]\n",
    "\n",
    "latest_icp = list(icp_profiles.aggregate(pipeline))\n",
    "if latest_icp:\n",
    "    latest_icp = latest_icp[0]\n",
    "    print(\"Latest active ICP:\")\n",
    "    print(latest_icp)\n",
    "else:\n",
    "    print(\"No active ICP found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27a92a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('6899cb3aafa62213309e0357'),\n",
       " 'name': 'google',\n",
       " 'filters': {'locations': ['usa', 'canada'],\n",
       "  'tech_stack': ['AWS', 'python'],\n",
       "  'keywords': ['ai, ml'],\n",
       "  'designations': ['hr'],\n",
       "  'department': ['tect', 'aq', 'it'],\n",
       "  'industry': ['software', 'ai'],\n",
       "  'employee_count': {'min': 1, 'max': 500},\n",
       "  'founded_after': 2019},\n",
       " 'tags': ['tech', 'software'],\n",
       " 'active': True,\n",
       " 'created_at': datetime.datetime(2025, 8, 11, 10, 51, 38, 658000),\n",
       " 'updated_at': datetime.datetime(2025, 8, 11, 10, 51, 38, 658000),\n",
       " 'effective_time': datetime.datetime(2025, 8, 11, 10, 51, 38, 658000)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_icp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50901ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('68890042ada3746b1bacf2a5'), 'name': 'Flowcast AI solutions', 'domain': 'flowcast.ai', 'industries': ['Artificial Intelligence', 'FinTech'], 'employee_count_estimate': {'min': 50, 'max': 200}, 'hq_location': {'city': 'San Francisco', 'country': 'USA'}, 'founded_year': 2019, 'tech_stack': ['Python', 'AWS', 'LangChain'], 'social_metrics': {'followers': {'linkedin': 1500, 'twitter': 600}}, 'source_urls': ['https://linkedin.com/company/flowcast-ai'], 'last_scraped': datetime.datetime(2025, 7, 27, 11, 0), 'last_scored': None, 'last_vectorized': datetime.datetime(2025, 8, 4, 10, 32, 19, 574000), 'changed_at': datetime.datetime(2025, 8, 3, 10, 0), 'icp_id': ObjectId('64c4a23b69a84f2b0b789cde'), 'icp_version': 1}\n",
      "{'_id': ObjectId('68890042ada3746b1bacf2a6'), 'name': 'SynthAI Inc.', 'domain': 'ssynthai.io', 'industries': ['Artificial Intelligence', 'SaaS'], 'employee_count_estimate': {'min': 20, 'max': 80}, 'hq_location': {'city': 'Toronto', 'country': 'Canada'}, 'founded_year': 2021, 'tech_stack': ['Python', 'GCP', 'HuggingFace'], 'social_metrics': {'followers': {'linkedin': 700, 'twitter': 1200}}, 'source_urls': ['https://linkedin.com/company/synthai'], 'last_scraped': datetime.datetime(2025, 7, 28, 8, 30), 'last_scored': datetime.datetime(2025, 7, 27, 12, 0), 'last_vectorized': datetime.datetime(2025, 8, 4, 10, 32, 19, 593000), 'changed_at': datetime.datetime(2025, 7, 27, 13, 0), 'icp_id': ObjectId('64c4a23b69a84f2b0b789cdf'), 'icp_version': 1}\n",
      "{'_id': ObjectId('68890042ada3746b1bacf2a7'), 'name': 'DocStream AI', 'domain': 'ddocstream.com', 'industries': ['SaaS', 'Document Management'], 'employee_count_estimate': {'min': 100, 'max': 250}, 'hq_location': {'city': 'Austin', 'country': 'USA'}, 'founded_year': 2016, 'tech_stack': ['Node.js', 'MongoDB', 'AWS'], 'social_metrics': {'followers': {'linkedin': 300, 'twitter': 90}}, 'source_urls': ['https://linkedin.com/company/docstream'], 'last_scraped': datetime.datetime(2025, 7, 25, 9, 0), 'last_scored': datetime.datetime(2025, 7, 26, 9, 30), 'last_vectorized': datetime.datetime(2025, 8, 4, 10, 32, 19, 613000), 'changed_at': datetime.datetime(2025, 7, 25, 14, 0), 'icp_id': ObjectId('64c4a23b69a84f2b0b789ce0'), 'icp_version': 1}\n",
      "{'_id': ObjectId('68890042ada3746b1bacf2a8'), 'name': 'NeoGen Cloud', 'domain': 'neogencloud.com', 'industries': ['Cloud', 'AI'], 'employee_count_estimate': {'min': 5, 'max': 30}, 'hq_location': {'city': 'Vancouver', 'country': 'Canada'}, 'founded_year': 2022, 'tech_stack': ['Azure', 'Docker', 'LangChain'], 'social_metrics': {'followers': {'linkedin': 250, 'twitter': 50}}, 'source_urls': ['https://linkedin.com/company/neogencloud'], 'last_scraped': datetime.datetime(2025, 7, 29, 9, 0), 'last_scored': None, 'last_vectorized': datetime.datetime(2025, 8, 4, 10, 32, 19, 630000), 'changed_at': datetime.datetime(2025, 7, 29, 8, 50), 'icp_id': ObjectId('64c4a23b69a84f2b0b789ce1'), 'icp_version': 2}\n",
      "{'_id': ObjectId('68890042ada3746b1bacf2a9'), 'name': 'Retico AI', 'domain': 'retico.ai', 'industries': ['Artificial Intelligence'], 'employee_count_estimate': {'min': 500, 'max': 1000}, 'hq_location': {'city': 'New York', 'country': 'USA'}, 'founded_year': 2015, 'tech_stack': ['Python', 'AWS', 'Docker'], 'social_metrics': {'followers': {'linkedin': 5000, 'twitter': 3000}}, 'source_urls': ['https://linkedin.com/company/retico'], 'last_scraped': datetime.datetime(2025, 7, 28, 12, 0), 'last_scored': datetime.datetime(2025, 7, 27, 12, 0), 'last_vectorized': datetime.datetime(2025, 8, 4, 10, 32, 19, 644000), 'changed_at': datetime.datetime(2025, 7, 28, 12, 0), 'icp_id': ObjectId('64c4a23b69a84f2b0b789ce2'), 'icp_version': 3}\n"
     ]
    }
   ],
   "source": [
    "for i in discovered_companies.find({}):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04317eb",
   "metadata": {},
   "source": [
    "#### Delta Ware Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c94e718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total <company, ICP> pairs to score: 5\n"
     ]
    }
   ],
   "source": [
    "latest_icp = list(icp_profiles.aggregate(pipeline))\n",
    "companies_to_score = []\n",
    "\n",
    "for icp in latest_icp:\n",
    "    icp_id = str(icp[\"_id\"])\n",
    "    icp_version = icp.get(\"version\", 1)\n",
    "\n",
    "    for company in discovered_companies.find({}):\n",
    "        company_id = company[\"_id\"]\n",
    "        last_scraped = company[\"last_scraped\"]\n",
    "\n",
    "\n",
    "        # Look up the scored company metadata from separate collection\n",
    "        score_doc = scored_companies.find_one({\n",
    "            \"company_id\": company_id,\n",
    "            \"icp_id\": icp_id\n",
    "        })\n",
    "\n",
    "        # CASE A: Never scored for this ICP\n",
    "        if not score_doc:\n",
    "            companies_to_score.append({\"company\": company, \"icp\": icp})\n",
    "            continue\n",
    "\n",
    "        # CASE B: Rescraped after last scoring\n",
    "        last_scored = score_doc.get(\"last_scored\")\n",
    "        if last_scored is None or last_scored > last_scraped: # logic changed just to check, reverse it once the task is done\n",
    "            companies_to_score.append({\"company\": company, \"icp\": icp})\n",
    "            continue\n",
    "\n",
    "        # CASE C: ICP version has changed\n",
    "        scored_version = score_doc.get(\"icp_version\")\n",
    "        if scored_version != icp_version:\n",
    "            companies_to_score.append({\"company\": company, \"icp\": icp})\n",
    "            continue\n",
    "\n",
    "print(f\"Total <company, ICP> pairs to score: {len(companies_to_score)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc369b8",
   "metadata": {},
   "source": [
    "### Collections Creation in Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d95d04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === First: Create collection if it doesn't exist ===\n",
    "\n",
    "\n",
    "# if qdrant.collection_exists(\"icp_vectors\"):\n",
    "#     qdrant.delete_collection(\"icp_vectors\")\n",
    "\n",
    "# ICP_Vectors\n",
    "\n",
    "if not qdrant.collection_exists(\"icp_vectors\"):\n",
    "    qdrant.create_collection(\n",
    "        collection_name=\"icp_vectors\",\n",
    "        vectors_config=VectorParams(size=384, distance=Distance.COSINE)  # 384 for MiniLM\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# if qdrant.collection_exists(\"company_vectors\"):\n",
    "#     qdrant.delete_collection(\"company_vectors\")\n",
    "\n",
    "# Company_Vectors\n",
    "\n",
    "if not qdrant.collection_exists(\"company_vectors\"):\n",
    "    qdrant.create_collection(\n",
    "        collection_name=\"company_vectors\",\n",
    "        vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506967b",
   "metadata": {},
   "source": [
    "#### Company Vectorization and upsert with filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77effc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# import numpy as np\n",
    "# import uuid\n",
    "# from pymongo import UpdateOne\n",
    "# from qdrant_client.models import PointStruct\n",
    "# from bson import ObjectId\n",
    "\n",
    "# def normalize_list(value):\n",
    "#     if isinstance(value, str):\n",
    "#         return [value.lower()]\n",
    "#     if isinstance(value, list):\n",
    "#         return list({v.strip().lower() for v in value if isinstance(v, str)})\n",
    "#     return []\n",
    "\n",
    "# def prepare_vector_text(company):\n",
    "#     parts = []\n",
    "#     parts += normalize_list(company.get(\"industries\"))\n",
    "#     parts += normalize_list(company.get(\"tech_stack\"))\n",
    "\n",
    "#     country = company.get(\"hq_location\", {}).get(\"country\", \"\").lower()\n",
    "#     if country:\n",
    "#         parts.append(country)\n",
    "\n",
    "#     return \" \".join(parts).strip()\n",
    "\n",
    "# points = []\n",
    "# bulk_updates = []\n",
    "\n",
    "# for pair in companies_to_score:\n",
    "#     company = pair[\"company\"]\n",
    "#     icp = pair[\"icp\"]\n",
    "\n",
    "#     domain = company.get(\"domain\")\n",
    "#     if not domain:\n",
    "#         continue\n",
    "\n",
    "#     # changed_at = company.get(\"changed_at\")\n",
    "#     # last_vectorized = company.get(\"last_vectorized\")\n",
    "\n",
    "#     # if changed_at and last_vectorized:\n",
    "#     #     if changed_at < last_vectorized:\n",
    "#     #         print(f\"Skipping {domain}: No changes since last vectorization.\")\n",
    "#     #         print(f\"[DEBUG] {domain} | changed_at: {changed_at}, last_vectorized: {last_vectorized}\")\n",
    "\n",
    "#     #         continue\n",
    "\n",
    "#     vector_text = prepare_vector_text(company)\n",
    "#     if not vector_text:\n",
    "#         print(f\"Skipping {domain}: No vector text.\")\n",
    "#         continue\n",
    "\n",
    "#     try:\n",
    "#         embedding = model.encode(vector_text)\n",
    "#         if embedding is None or not isinstance(embedding, (list, np.ndarray)) or np.all(np.array(embedding) == 0):\n",
    "#             print(f\"Skipping {domain}: Invalid embedding.\")\n",
    "#             continue\n",
    "\n",
    "#         embedding = np.array(embedding, dtype=np.float32).tolist()\n",
    "#         point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"company:{domain}\"))\n",
    "\n",
    "#         point = PointStruct(\n",
    "#             id=point_id,\n",
    "#             vector=embedding,\n",
    "#             payload={\n",
    "#                 \"company_id\": str(company.get(\"_id\")),\n",
    "#                 \"icp_id\": str(company.get(\"icp_id\", \"\")),\n",
    "#                 \"industry\": normalize_list(company.get(\"industries\")),\n",
    "#                 \"location\": company.get(\"hq_location\", {}).get(\"country\", \"\").lower(),\n",
    "#                 \"tech_stack\": normalize_list(company.get(\"tech_stack\"))\n",
    "#             }\n",
    "#         )\n",
    "#         points.append(point)\n",
    "\n",
    "#         bulk_updates.append(UpdateOne(\n",
    "#             {\"_id\": company[\"_id\"]},\n",
    "#             {\"$set\": {\"last_vectorized\": datetime.datetime.now(datetime.timezone.utc)}}\n",
    "#         ))\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error vectorizing {domain}: {e}\")\n",
    "\n",
    "# if points:\n",
    "#     qdrant.upsert(collection_name=\"company_vectors\", points=points)\n",
    "#     print(f\"Upserted {len(points)} companies to Qdrant.\")\n",
    "# else:\n",
    "#     print(\"No valid company vectors to upsert.\")\n",
    "\n",
    "# if bulk_updates:\n",
    "#     db[\"discovered_companies\"].bulk_write(bulk_updates)\n",
    "#     print(f\"Updated last_vectorized for {len(bulk_updates)} companies.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3133b59",
   "metadata": {},
   "source": [
    "#### ICP Vectorization and upsert with filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5327e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# from uuid import uuid5, NAMESPACE_DNS\n",
    "# import numpy as np\n",
    "# from qdrant_client.models import PointStruct\n",
    "\n",
    "# # === Helper: Normalize and join values ===\n",
    "# def normalize_list(value):\n",
    "#     if isinstance(value, str):\n",
    "#         return [value.lower()]\n",
    "#     if isinstance(value, list):\n",
    "#         return list({v.strip().lower() for v in value if isinstance(v, str)})\n",
    "#     return []\n",
    "\n",
    "# # === Load active ICPs ===\n",
    "# latest_icp = list(icp_profiles.aggregate(pipeline))\n",
    "# icp_points = []\n",
    "\n",
    "# for icp in latest_icp:\n",
    "#     icp_id = str(icp[\"_id\"])\n",
    "#     filters = icp.get(\"filters\", {})\n",
    "#     updated_at = icp.get(\"updated_at\", datetime.min)\n",
    "#     last_vectorized = icp.get(\"last_vectorized\")  # May be None\n",
    "\n",
    "#     # === Skip if already vectorized and not updated ===\n",
    "#     # if last_vectorized:\n",
    "#     #     if updated_at < last_vectorized:\n",
    "#     #         print(f\"Skipping ICP {icp_id}: No changes since last vectorization.\")\n",
    "#     #         continue\n",
    "\n",
    "#     # === Normalize and build vector text ===\n",
    "#     text_parts = []\n",
    "\n",
    "#     industries = normalize_list(filters.get(\"industries\", []))\n",
    "#     locations = normalize_list(filters.get(\"locations\", []))\n",
    "#     tech_stack = normalize_list(icp.get(\"tech_stack\", []))\n",
    "#     keywords = normalize_list(icp.get(\"keywords\", []))\n",
    "\n",
    "#     text_parts += industries + locations + tech_stack + keywords\n",
    "#     vector_text = \" \".join(text_parts).strip()\n",
    "\n",
    "#     if not vector_text:\n",
    "#         continue\n",
    "\n",
    "#     try:\n",
    "#         embedding = model.encode(vector_text)\n",
    "#         if embedding is None or not isinstance(embedding, (list, np.ndarray)) or np.all(np.array(embedding) == 0):\n",
    "#             continue\n",
    "\n",
    "#         embedding = np.array(embedding, dtype=np.float32).tolist()\n",
    "#         point_id = str(uuid5(NAMESPACE_DNS, f\"icp:{icp_id}\"))\n",
    "\n",
    "#         point = PointStruct(\n",
    "#             id=point_id,\n",
    "#             vector=embedding,\n",
    "#             payload={\n",
    "#                 \"icp_id\": icp_id,\n",
    "#                 \"industry\": industries,\n",
    "#                 \"location\": locations,\n",
    "#                 \"tech_stack\": tech_stack,\n",
    "#                 \"keywords\": keywords\n",
    "#             }\n",
    "#         )\n",
    "#         icp_points.append(point)\n",
    "\n",
    "#         # === Update last_vectorized timestamp in MongoDB ===\n",
    "#         # icp_profiles.update_one(\n",
    "#         #     {\"_id\": icp[\"_id\"]},\n",
    "#         #     {\"$set\": {\"last_vectorized\": datetime.utcnow()}}\n",
    "#         # )\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error encoding ICP {icp_id}: {e}\")\n",
    "\n",
    "# # === Final upsert to Qdrant ===\n",
    "# if icp_points:\n",
    "#     qdrant.upsert(collection_name=\"icp_vectors\", points=icp_points)\n",
    "#     print(f\"Upserted {len(icp_points)} ICP profiles to Qdrant.\")\n",
    "# else:\n",
    "#     print(\"No valid ICP vectors to upsert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f801d",
   "metadata": {},
   "source": [
    "#### without filter ICP and Company Vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cc04458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagar/pipeline OS/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 5 companies to Qdrant.\n"
     ]
    }
   ],
   "source": [
    "# === Helper: Normalize and join string lists ===\n",
    "def normalize_list(value):\n",
    "    if isinstance(value, str):\n",
    "        return [value.lower()]\n",
    "    if isinstance(value, list):\n",
    "        return list({v.strip().lower() for v in value if isinstance(v, str)})\n",
    "    return []\n",
    "\n",
    "# === Helper: Prepare vector text from normalized fields ===\n",
    "def prepare_vector_text(company):\n",
    "    parts = []\n",
    "\n",
    "    industries = normalize_list(company.get(\"industries\"))\n",
    "    tech_stack = normalize_list(company.get(\"tech_stack\"))\n",
    "    country = company.get(\"hq_location\", {}).get(\"country\", \"\").lower()\n",
    "\n",
    "    parts += industries\n",
    "    parts += tech_stack\n",
    "    if country:\n",
    "        parts.append(country)\n",
    "\n",
    "    return \" \".join(parts).strip()\n",
    "\n",
    "# === Vectorize and Upsert to Qdrant ===\n",
    "points = []\n",
    "for pair in companies_to_score:\n",
    "    company = pair[\"company\"]\n",
    "    icp = pair[\"icp\"]\n",
    "\n",
    "    domain = company.get(\"domain\")\n",
    "    if not domain:\n",
    "        continue\n",
    "\n",
    "    # === Check vector freshness ===\n",
    "    # changed_at = company.get(\"changed_at\")\n",
    "    # last_vectorized = company.get(\"last_vectorized\")\n",
    "\n",
    "    # if changed_at and last_vectorized:\n",
    "    #     try:\n",
    "    #         if isinstance(changed_at, str):\n",
    "    #             changed_at = datetime.datetime(changed_at)\n",
    "    #         if isinstance(last_vectorized, str):\n",
    "    #             last_vectorized = datetime.datetime(last_vectorized)\n",
    "\n",
    "    #         if changed_at <= last_vectorized:\n",
    "    #             print(f\"Skipping {domain}: No changes since last vectorization.\")\n",
    "    #             continue\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Date parse error for {domain}: {e}\")\n",
    "\n",
    "    vector_text = prepare_vector_text(company)\n",
    "    if not vector_text:\n",
    "        print(f\"Skipping {domain}: No vector text.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        embedding = model.encode(vector_text)\n",
    "        if embedding is None or not isinstance(embedding, (list, np.ndarray)) or np.all(np.array(embedding) == 0):\n",
    "            print(f\"Skipping {domain}: Invalid embedding.\")\n",
    "            continue\n",
    "\n",
    "        embedding = np.array(embedding, dtype=np.float32).tolist()\n",
    "        point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"company:{domain}\"))\n",
    "\n",
    "        point = PointStruct(\n",
    "            id=point_id,\n",
    "            vector=embedding,\n",
    "            payload={\n",
    "                \"company_id\": str(company.get(\"_id\")),\n",
    "                \"icp_id\": str(company.get(\"icp_id\", \"\")),\n",
    "                \"industry\": normalize_list(company.get(\"industries\")),\n",
    "                \"location\": company.get(\"hq_location\", {}).get(\"country\", \"\").lower(),\n",
    "                \"tech_stack\": normalize_list(company.get(\"tech_stack\"))\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "    except Exception as e:\n",
    "        print(f\"Error vectorizing {domain}: {e}\")\n",
    "\n",
    "# === Upsert all vectors into Qdrant ===\n",
    "if points:\n",
    "    qdrant.upsert(collection_name=\"company_vectors\", points=points)\n",
    "    print(f\"Upserted {len(points)} companies to Qdrant.\")\n",
    "else:\n",
    "    print(\"No valid company vectors to upsert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb351fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6899cb3aafa62213309e0357] vector_text: 'ai software usa canada python aws' | filters: {'locations': ['usa', 'canada'], 'tech_stack': ['AWS', 'python'], 'keywords': ['ai, ml'], 'designations': ['hr'], 'department': ['tect', 'aq', 'it'], 'industry': ['software', 'ai'], 'employee_count': {'min': 1, 'max': 500}, 'founded_after': 2019}\n",
      "Upserted 1 ICP profiles to Qdrant.\n"
     ]
    }
   ],
   "source": [
    "# === Helper: Normalize and join values ===\n",
    "def normalize_list(value):\n",
    "    if isinstance(value, str):\n",
    "        return [value.lower()]\n",
    "    if isinstance(value, list):\n",
    "        return list({v.strip().lower() for v in value if isinstance(v, str)})\n",
    "    return []\n",
    "\n",
    "# === Load active ICPs ===\n",
    "latest_icp = list(icp_profiles.aggregate(pipeline))\n",
    "icp_points = []\n",
    "\n",
    "for icp in latest_icp:\n",
    "    icp_id = str(icp[\"_id\"])\n",
    "    filters = icp.get(\"filters\", {})\n",
    "    text_parts = []\n",
    "\n",
    "    # Normalize and extract fields\n",
    "    # FIXED\n",
    "    industry = normalize_list(filters.get(\"industry\", []))\n",
    "    locations = normalize_list(filters.get(\"locations\", []))\n",
    "    tech_stack = normalize_list(filters.get(\"tech_stack\", []))\n",
    "\n",
    "\n",
    "    text_parts += industry + locations + tech_stack\n",
    "    vector_text = \" \".join(text_parts).strip()\n",
    "\n",
    "    if not vector_text:\n",
    "        continue\n",
    "    print(f\"[{icp_id}] vector_text: '{vector_text}' | filters: {filters}\")\n",
    "\n",
    "    try:\n",
    "        embedding = model.encode(vector_text)\n",
    "        if embedding is None or not isinstance(embedding, (list, np.ndarray)) or np.all(np.array(embedding) == 0):\n",
    "            continue\n",
    "\n",
    "        embedding = np.array(embedding, dtype=np.float32).tolist()\n",
    "        point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"icp:{icp_id}\"))\n",
    "\n",
    "        point = PointStruct(\n",
    "            id=point_id,\n",
    "            vector=embedding,\n",
    "            payload={\n",
    "                \"icp_id\": icp_id,\n",
    "                \"industry\": industry,\n",
    "                \"location\": locations,\n",
    "                \"tech_stack\": tech_stack\n",
    "            }\n",
    "        )\n",
    "        icp_points.append(point)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding ICP {icp_id}: {e}\")\n",
    "\n",
    "# === Upsert to Qdrant ===\n",
    "if icp_points:\n",
    "    qdrant.upsert(collection_name=\"icp_vectors\", points=icp_points)\n",
    "    print(f\"Upserted {len(icp_points)} ICP profiles to Qdrant.\")\n",
    "else:\n",
    "    print(\"No valid ICP vectors to upsert.\")\n",
    "    print(f\"[{icp_id}] vector_text: '{vector_text}' | filters: {filters}\")\n",
    "    print(f\"[{icp_id}] Embedding generated: {embedding is not None} | Zero vector: {np.all(np.array(embedding)==0) if embedding is not None else 'N/A'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d671d03e",
   "metadata": {},
   "source": [
    "#### cos_sim scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfbaacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEIGHTS = {\n",
    "#     \"industry\": 20,\n",
    "#     \"employee_count\": 15,\n",
    "#     \"location\": 15,\n",
    "#     \"tech_stack\": 25,\n",
    "#     \"keywords\": 10,\n",
    "#     \"founded_year\": 5,\n",
    "#     \"github_signal\": 10\n",
    "# }\n",
    " \n",
    "# # --- 1. Get latest active ICP ---\n",
    "# latest_icp = list(icp_profiles.aggregate(pipeline))\n",
    " \n",
    "# if not latest_icp:\n",
    "#     print(\"❌ No active ICP found.\")\n",
    "#     exit()\n",
    "# icp = latest_icp[0]\n",
    "# icp_id = str(icp[\"_id\"])\n",
    "# icp_version = icp.get(\"version\", 1)\n",
    " \n",
    "# # --- 2. Get ICP vector from Qdrant ---\n",
    " \n",
    "# icp_vector_resp = qdrant.retrieve(\n",
    "#     collection_name=\"icp_vectors\",\n",
    "#     ids= [str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"icp:{icp_id}\"))],\n",
    "#     with_vectors=True\n",
    "# )\n",
    " \n",
    "# if not icp_vector_resp or not icp_vector_resp[0].vector:\n",
    "#     print(\"❌ ICP vector not found.\")\n",
    "#     exit()\n",
    " \n",
    "# icp_embedding = icp_vector_resp[0].vector\n",
    " \n",
    "# # --- 3. Get all company vectors from Qdrant ---\n",
    "# search_result = qdrant.query_points(\n",
    "#     collection_name=\"company_vectors\",\n",
    "#     query=icp_embedding,\n",
    "#     limit=10000,\n",
    "#     with_payload=True,\n",
    "#     with_vectors=True\n",
    "# )\n",
    "# print(f\"Total points returned from Qdrant: {len(search_result.points)}\")\n",
    " \n",
    "# icp_founded_after = icp.get(\"filters\", {}).get(\"founded_after\", 2015)\n",
    " \n",
    "# # --- 4. Scoring Rule Function () ---\n",
    "# def rule_score(company_doc, icp_tokens, icp_founded_after):\n",
    "#     rule_score = 0\n",
    "#     breakdown = {k: 0 for k in WEIGHTS}\n",
    "#     def safe_list(field):\n",
    "#         value = company_doc.get(field, [])\n",
    "#         if isinstance(value, str): return [value]\n",
    "#         if isinstance(value, list): return value\n",
    "#         return []\n",
    "    \n",
    "#     # Match tokens in industry, location, tech stack, keywords\n",
    "#     for field, key in [(\"industries\", \"industry\"), (\"hq_location\", \"location\"),\n",
    "#                        (\"tech_stack\", \"tech_stack\"), (\"keywords\", \"keywords\")]:\n",
    "#         values = (\n",
    "#             safe_list(field) if field != \"hq_location\"\n",
    "#             else list(company_doc.get(\"hq_location\", {}).values())\n",
    "#         )\n",
    "#         if any(token in str(v).lower() for v in values for token in icp_tokens):\n",
    "#             rule_score += WEIGHTS[key]\n",
    "#             breakdown[key] = WEIGHTS[key]\n",
    " \n",
    "#     # Match employee count\n",
    "#     emp = company_doc.get(\"employee_count_estimate\", {})\n",
    "#     if emp.get(\"min\", 0) >= 10 and emp.get(\"max\", 0) <= 500:\n",
    "#         rule_score += WEIGHTS[\"employee_count\"]\n",
    "#         breakdown[\"employee_count\"] = WEIGHTS[\"employee_count\"]\n",
    " \n",
    "#     # Founded year match\n",
    "#     if isinstance(company_doc.get(\"founded_year\"), int) and company_doc[\"founded_year\"] >= icp_founded_after:\n",
    "#         rule_score += WEIGHTS[\"founded_year\"]\n",
    "#         breakdown[\"founded_year\"] = WEIGHTS[\"founded_year\"]\n",
    " \n",
    "#     # GitHub presence\n",
    "#     urls = company_doc.get(\"source_urls\", [])\n",
    "#     if any(\"github\" in url.lower() for url in urls):\n",
    "#         rule_score += WEIGHTS[\"github_signal\"]\n",
    "#         breakdown[\"github_signal\"] = WEIGHTS[\"github_signal\"]\n",
    " \n",
    "#     return rule_score, breakdown\n",
    " \n",
    "# # --- 5. Tokenize ICP filters ---\n",
    "# icp_tokens = [token.lower().strip() for token in f\"{' '.join(icp.get('filters', {}).get('industries', []))} {' '.join(icp.get('filters', {}).get('locations', []))}\".split()]\n",
    " \n",
    "# # --- 6. Score companies using util.cos_sim() ---\n",
    "# scored_results_util = []\n",
    "# icp_tensor = torch.tensor(model.encode([' '.join(icp_tokens)]), dtype=torch.float32)  # Convert ICP to tensor\n",
    " \n",
    "# for point in search_result.points:\n",
    "#     payload = point.payload\n",
    "#     company_id = payload.get(\"company_id\")\n",
    "#     company_vector = point.vector\n",
    "    \n",
    "#     # Calculate cosine similarity using util.cos_sim()\n",
    "#     company_tensor = torch.tensor([company_vector], dtype=torch.float32)\n",
    "#     similarity = util.cos_sim(icp_tensor, company_tensor)[0][0].item()\n",
    "    \n",
    "#     try:\n",
    "#         if isinstance(company_id, str):\n",
    "#             company_id = ObjectId(company_id)\n",
    "#     except Exception as e:\n",
    "#         print(f'{company_id} have {e}')\n",
    "#         continue \n",
    " \n",
    "#     company = discovered_companies.find_one({\"_id\": company_id})\n",
    "#     if not company:\n",
    "#         continue\n",
    "        \n",
    "#     rule, breakdown = rule_score(company, icp_tokens, icp_founded_after)\n",
    "#     if rule == 0:\n",
    "#         final_score = round(similarity * 0.1, 4)\n",
    "#     else:\n",
    "#         final_score = round(similarity * rule, 4)\n",
    "    \n",
    "#     breakdown[\"vector_similarity\"] = round(similarity, 4)\n",
    " \n",
    "#     scored_doc = {\n",
    "#         \"company_id\": company_id,\n",
    "#         \"icp_id\": icp_id,\n",
    "#         \"icp_version\": icp_version,\n",
    "#         \"final_score\": final_score,\n",
    "#         \"breakdown\": breakdown,\n",
    "#         \"weights\": WEIGHTS,\n",
    "#         \"last_scored\": datetime.now(timezone.utc),\n",
    "#         \"method\": \"util_cos_sim\"\n",
    "#     }\n",
    "    \n",
    "#     # Store in a different collection or add method flag\n",
    "#     scored_companies.update_one(\n",
    "#         {\"company_id\": company_id, \"icp_id\": icp_id, \"method\": \"util_cos_sim\"},\n",
    "#         {\"$set\": scored_doc},\n",
    "#         upsert=True\n",
    "#     )\n",
    "#     scored_results_util.append((company.get(\"domain\", \"unknown\"), final_score))\n",
    " \n",
    "# # --- 7. Print Top Results ---\n",
    "# print(\"Scoring complete using util.cos_sim(). Top results:\")\n",
    "# for domain, score in sorted(scored_results_util, key=lambda x: x[1], reverse=True)[:20]:\n",
    "#     print(f\"{domain} → Score: {score}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de7186",
   "metadata": {},
   "source": [
    "## dicovered_comanies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3550e989",
   "metadata": {},
   "source": [
    "{\n",
    "  \"_id\": {\n",
    "    \"$oid\": \"68890042ada3746b1bacf2a5\"\n",
    "  },\n",
    "  \"name\": \"Flowcast AI solutions\",\n",
    "  \"domain\": \"flowcast.ai\",\n",
    "  \"industries\": [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"FinTech\"\n",
    "  ],\n",
    "  \"employee_count_estimate\": {\n",
    "    \"min\": 50,\n",
    "    \"max\": 200\n",
    "  },\n",
    "  \"hq_location\": {\n",
    "    \"city\": \"San Francisco\",\n",
    "    \"country\": \"USA\"\n",
    "  },\n",
    "  \"founded_year\": 2019,\n",
    "  \"tech_stack\": [\n",
    "    \"Python\",\n",
    "    \"AWS\",\n",
    "    \"LangChain\"\n",
    "  ],\n",
    "  \"social_metrics\": {\n",
    "    \"followers\": {\n",
    "      \"linkedin\": 1500,\n",
    "      \"twitter\": 600\n",
    "    }\n",
    "  },\n",
    "  \"source_urls\": [\n",
    "    \"https://linkedin.com/company/flowcast-ai\"\n",
    "  ],\n",
    "  \"last_scraped\": {\n",
    "    \"$date\": \"2025-07-27T11:00:00.000Z\"\n",
    "  },\n",
    "  \"last_scored\": null,\n",
    "  \"last_vectorized\": {\n",
    "    \"$date\": \"2025-08-04T10:32:19.574Z\"\n",
    "  },\n",
    "  \"changed_at\": {\n",
    "    \"$date\": \"2025-08-03T10:00:00.000Z\"\n",
    "  },\n",
    "  \"icp_id\": {\n",
    "    \"$oid\": \"64c4a23b69a84f2b0b789cde\"\n",
    "  },\n",
    "  \"icp_version\": 1\n",
    "}\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "{\n",
    "  \"_id\": {\n",
    "    \"$oid\": \"68890042ada3746b1bacf2a6\"\n",
    "  },\n",
    "  \"name\": \"SynthAI Inc.\",\n",
    "  \"domain\": \"ssynthai.io\",\n",
    "  \"industries\": [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"SaaS\"\n",
    "  ],\n",
    "  \"employee_count_estimate\": {\n",
    "    \"min\": 20,\n",
    "    \"max\": 80\n",
    "  },\n",
    "  \"hq_location\": {\n",
    "    \"city\": \"Toronto\",\n",
    "    \"country\": \"Canada\"\n",
    "  },\n",
    "  \"founded_year\": 2021,\n",
    "  \"tech_stack\": [\n",
    "    \"Python\",\n",
    "    \"GCP\",\n",
    "    \"HuggingFace\"\n",
    "  ],\n",
    "  \"social_metrics\": {\n",
    "    \"followers\": {\n",
    "      \"linkedin\": 700,\n",
    "      \"twitter\": 1200\n",
    "    }\n",
    "  },\n",
    "  \"source_urls\": [\n",
    "    \"https://linkedin.com/company/synthai\"\n",
    "  ],\n",
    "  \"last_scraped\": {\n",
    "    \"$date\": \"2025-07-28T08:30:00.000Z\"\n",
    "  },\n",
    "  \"last_scored\": {\n",
    "    \"$date\": \"2025-07-27T12:00:00.000Z\"\n",
    "  },\n",
    "  \"last_vectorized\": {\n",
    "    \"$date\": \"2025-08-04T10:32:19.593Z\"\n",
    "  },\n",
    "  \"changed_at\": {\n",
    "    \"$date\": \"2025-07-27T13:00:00.000Z\"\n",
    "  },\n",
    "  \"icp_id\": {\n",
    "    \"$oid\": \"64c4a23b69a84f2b0b789cdf\"\n",
    "  },\n",
    "  \"icp_version\": 1\n",
    "}\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "{\n",
    "  \"_id\": {\n",
    "    \"$oid\": \"68890042ada3746b1bacf2a7\"\n",
    "  },\n",
    "  \"name\": \"DocStream AI\",\n",
    "  \"domain\": \"ddocstream.com\",\n",
    "  \"industries\": [\n",
    "    \"SaaS\",\n",
    "    \"Document Management\"\n",
    "  ],\n",
    "  \"employee_count_estimate\": {\n",
    "    \"min\": 100,\n",
    "    \"max\": 250\n",
    "  },\n",
    "  \"hq_location\": {\n",
    "    \"city\": \"Austin\",\n",
    "    \"country\": \"USA\"\n",
    "  },\n",
    "  \"founded_year\": 2016,\n",
    "  \"tech_stack\": [\n",
    "    \"Node.js\",\n",
    "    \"MongoDB\",\n",
    "    \"AWS\"\n",
    "  ],\n",
    "  \"social_metrics\": {\n",
    "    \"followers\": {\n",
    "      \"linkedin\": 300,\n",
    "      \"twitter\": 90\n",
    "    }\n",
    "  },\n",
    "  \"source_urls\": [\n",
    "    \"https://linkedin.com/company/docstream\"\n",
    "  ],\n",
    "  \"last_scraped\": {\n",
    "    \"$date\": \"2025-07-25T09:00:00.000Z\"\n",
    "  },\n",
    "  \"last_scored\": {\n",
    "    \"$date\": \"2025-07-26T09:30:00.000Z\"\n",
    "  },\n",
    "  \"last_vectorized\": {\n",
    "    \"$date\": \"2025-08-04T10:32:19.613Z\"\n",
    "  },\n",
    "  \"changed_at\": {\n",
    "    \"$date\": \"2025-07-25T14:00:00.000Z\"\n",
    "  },\n",
    "  \"icp_id\": {\n",
    "    \"$oid\": \"64c4a23b69a84f2b0b789ce0\"\n",
    "  },\n",
    "  \"icp_version\": 1\n",
    "}\n",
    "\n",
    "-----------------------\n",
    "\n",
    "{\n",
    "  \"_id\": {\n",
    "    \"$oid\": \"68890042ada3746b1bacf2a8\"\n",
    "  },\n",
    "  \"name\": \"NeoGen Cloud\",\n",
    "  \"domain\": \"neogencloud.com\",\n",
    "  \"industries\": [\n",
    "    \"Cloud\",\n",
    "    \"AI\"\n",
    "  ],\n",
    "  \"employee_count_estimate\": {\n",
    "    \"min\": 5,\n",
    "    \"max\": 30\n",
    "  },\n",
    "  \"hq_location\": {\n",
    "    \"city\": \"Vancouver\",\n",
    "    \"country\": \"Canada\"\n",
    "  },\n",
    "  \"founded_year\": 2022,\n",
    "  \"tech_stack\": [\n",
    "    \"Azure\",\n",
    "    \"Docker\",\n",
    "    \"LangChain\"\n",
    "  ],\n",
    "  \"social_metrics\": {\n",
    "    \"followers\": {\n",
    "      \"linkedin\": 250,\n",
    "      \"twitter\": 50\n",
    "    }\n",
    "  },\n",
    "  \"source_urls\": [\n",
    "    \"https://linkedin.com/company/neogencloud\"\n",
    "  ],\n",
    "  \"last_scraped\": {\n",
    "    \"$date\": \"2025-07-29T09:00:00.000Z\"\n",
    "  },\n",
    "  \"last_scored\": null,\n",
    "  \"last_vectorized\": {\n",
    "    \"$date\": \"2025-08-04T10:32:19.630Z\"\n",
    "  },\n",
    "  \"changed_at\": {\n",
    "    \"$date\": \"2025-07-29T08:50:00.000Z\"\n",
    "  },\n",
    "  \"icp_id\": {\n",
    "    \"$oid\": \"64c4a23b69a84f2b0b789ce1\"\n",
    "  },\n",
    "  \"icp_version\": 2\n",
    "}\n",
    "\n",
    "\n",
    "--------------------------\n",
    "\n",
    "{\n",
    "  \"_id\": {\n",
    "    \"$oid\": \"68890042ada3746b1bacf2a9\"\n",
    "  },\n",
    "  \"name\": \"Retico AI\",\n",
    "  \"domain\": \"retico.ai\",\n",
    "  \"industries\": [\n",
    "    \"Artificial Intelligence\"\n",
    "  ],\n",
    "  \"employee_count_estimate\": {\n",
    "    \"min\": 500,\n",
    "    \"max\": 1000\n",
    "  },\n",
    "  \"hq_location\": {\n",
    "    \"city\": \"New York\",\n",
    "    \"country\": \"USA\"\n",
    "  },\n",
    "  \"founded_year\": 2015,\n",
    "  \"tech_stack\": [\n",
    "    \"Python\",\n",
    "    \"AWS\",\n",
    "    \"Docker\"\n",
    "  ],\n",
    "  \"social_metrics\": {\n",
    "    \"followers\": {\n",
    "      \"linkedin\": 5000,\n",
    "      \"twitter\": 3000\n",
    "    }\n",
    "  },\n",
    "  \"source_urls\": [\n",
    "    \"https://linkedin.com/company/retico\"\n",
    "  ],\n",
    "  \"last_scraped\": {\n",
    "    \"$date\": \"2025-07-28T12:00:00.000Z\"\n",
    "  },\n",
    "  \"last_scored\": {\n",
    "    \"$date\": \"2025-07-27T12:00:00.000Z\"\n",
    "  },\n",
    "  \"last_vectorized\": {\n",
    "    \"$date\": \"2025-08-04T10:32:19.644Z\"\n",
    "  },\n",
    "  \"changed_at\": {\n",
    "    \"$date\": \"2025-07-28T12:00:00.000Z\"\n",
    "  },\n",
    "  \"icp_id\": {\n",
    "    \"$oid\": \"64c4a23b69a84f2b0b789ce2\"\n",
    "  },\n",
    "  \"icp_version\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222c51f",
   "metadata": {},
   "source": [
    "## Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e2d2bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total points returned from Qdrant: 5\n",
      "✅ Extracted ICP tokens: ['emp_min_1', 'founded_after_2019', 'usa', 'ai', 'software', 'ai, ml', 'canada', 'python', 'emp_max_500', 'aws']\n",
      "✅ Field-specific tokens: {'employee_count': {'min': 1, 'max': 500}, 'founded_after': 2019}\n",
      "\n",
      " {'industry': 0, 'employee_count': 15, 'location': 15, 'tech_stack': 25, 'keywords': 0, 'founded_year': 5, 'github_signal': 0}\n",
      "<function rule_score at 0x7717ca1b7640>\n",
      "\n",
      " {'industry': 0, 'employee_count': 0, 'location': 15, 'tech_stack': 25, 'keywords': 0, 'founded_year': 0, 'github_signal': 0}\n",
      "<function rule_score at 0x7717ca1b7640>\n",
      "\n",
      " {'industry': 0, 'employee_count': 15, 'location': 15, 'tech_stack': 25, 'keywords': 0, 'founded_year': 5, 'github_signal': 0}\n",
      "<function rule_score at 0x7717ca1b7640>\n",
      "\n",
      " {'industry': 20, 'employee_count': 0, 'location': 15, 'tech_stack': 25, 'keywords': 0, 'founded_year': 5, 'github_signal': 0}\n",
      "<function rule_score at 0x7717ca1b7640>\n",
      "\n",
      " {'industry': 0, 'employee_count': 15, 'location': 15, 'tech_stack': 25, 'keywords': 0, 'founded_year': 0, 'github_signal': 0}\n",
      "<function rule_score at 0x7717ca1b7640>\n",
      "📈 Total companies scored: 5\n",
      "\n",
      "🏆 TOP RESULTS (Weighted Average Method):\n",
      " 1. flowcast.ai          →  71.51%\n",
      " 2. neogencloud.com      →  69.71%\n",
      " 3. ssynthai.io          →  69.59%\n",
      " 4. ddocstream.com       →  59.94%\n",
      " 5. retico.ai            →  58.62%\n"
     ]
    }
   ],
   "source": [
    "WEIGHTS = {\n",
    "    \"industry\": 20,\n",
    "    \"employee_count\": 15,\n",
    "    \"location\": 15,\n",
    "    \"tech_stack\": 25,\n",
    "    \"keywords\": 10,\n",
    "    \"founded_year\": 5,\n",
    "    \"github_signal\": 10\n",
    "}\n",
    " \n",
    "# --- 1. Get latest active ICP ---\n",
    "latest_icp = list(icp_profiles.aggregate(pipeline))\n",
    " \n",
    "if not latest_icp:\n",
    "    print(\"❌ No active ICP found.\")\n",
    "    exit()\n",
    "icp = latest_icp[0]\n",
    "icp_id = str(icp[\"_id\"])\n",
    "icp_version = icp.get(\"version\", 1)\n",
    " \n",
    "# --- 2. Get ICP vector from Qdrant ---\n",
    " \n",
    "icp_vector_resp = qdrant.retrieve(\n",
    "    collection_name=\"icp_vectors\",\n",
    "    ids= [str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"icp:{icp_id}\"))],\n",
    "    with_vectors=True\n",
    ")\n",
    " \n",
    "if not icp_vector_resp or not icp_vector_resp[0].vector:\n",
    "    print(\"❌ ICP vector not found.\")\n",
    "    exit()\n",
    " \n",
    "icp_embedding = icp_vector_resp[0].vector\n",
    " \n",
    "# --- 3. Get all company vectors from Qdrant ---\n",
    "search_result = qdrant.query_points(\n",
    "    collection_name=\"company_vectors\",\n",
    "    query=icp_embedding,\n",
    "    limit=10000,\n",
    "    with_payload=True,\n",
    "    with_vectors=True\n",
    ")\n",
    "print(f\"Total points returned from Qdrant: {len(search_result.points)}\")\n",
    " \n",
    "# icp_founded_after = icp.get(\"filters\", {}).get(\"founded_after\", 2015)\n",
    " \n",
    "# --- 4. Scoring Rule Function () ---\n",
    "def rule_score(company_doc, icp_tokens, icp_field_tokens):\n",
    "    rule_score = 0\n",
    "    breakdown = {k: 0 for k in WEIGHTS}\n",
    "    def safe_list(field):\n",
    "        value = company_doc.get(field, [])\n",
    "        if isinstance(value, str): return [value]\n",
    "        if isinstance(value, list): return value\n",
    "        return []\n",
    "    \n",
    "    # Match tokens in industry, location, tech stack, keywords\n",
    "    for field, key in [(\"industries\", \"industry\"), (\"hq_location\", \"location\"),\n",
    "                       (\"tech_stack\", \"tech_stack\"), (\"keywords\", \"keywords\")]:\n",
    "        values = (\n",
    "            safe_list(field) if field != \"hq_location\"\n",
    "            else list(company_doc.get(\"hq_location\", {}).values())\n",
    "        )\n",
    "        if any(token in str(v).lower() for v in values for token in icp_tokens):\n",
    "            rule_score += WEIGHTS[key]\n",
    "            breakdown[key] = WEIGHTS[key]\n",
    " \n",
    "    # Match employee count\n",
    "    emp = company_doc.get(\"employee_count_estimate\", {})\n",
    "    if emp.get(\"min\", 0) >= 10 and emp.get(\"max\", 0) <= 500:\n",
    "        rule_score += WEIGHTS[\"employee_count\"]\n",
    "        breakdown[\"employee_count\"] = WEIGHTS[\"employee_count\"]\n",
    " \n",
    "    # Founded year match\n",
    "    if isinstance(company_doc.get(\"founded_year\"), int) and company_doc[\"founded_year\"] >= icp_field_tokens.get(\"founded_after\") :\n",
    "        rule_score += WEIGHTS[\"founded_year\"]\n",
    "        breakdown[\"founded_year\"] = WEIGHTS[\"founded_year\"]\n",
    " \n",
    "    # GitHub presence\n",
    "    urls = company_doc.get(\"source_urls\", [])\n",
    "    if any(\"github\" in url.lower() for url in urls):\n",
    "        rule_score += WEIGHTS[\"github_signal\"]\n",
    "        breakdown[\"github_signal\"] = WEIGHTS[\"github_signal\"]\n",
    " \n",
    "    return rule_score, breakdown\n",
    " \n",
    "# --- 5. Tokenize ICP filters ---\n",
    "icp_filters = icp.get('filters', {})\n",
    "icp_tokens =[]\n",
    "icp_field_tokens = {}\n",
    "\n",
    "# extract industry tokens\n",
    "for field_name in ['industry', 'industries']:\n",
    "    if field_name in icp_filters:\n",
    "        industries = icp_filters[field_name]\n",
    "        if isinstance(industries, list):\n",
    "            for industry in industries:\n",
    "                tokens = str(industry).lower().split()\n",
    "                icp_tokens.extend(tokens)\n",
    "        elif isinstance(industries, str):\n",
    "            tokens = industries.lower().split()\n",
    "            icp_tokens.extend(tokens)\n",
    "\n",
    "# Extract tech stack tokens\n",
    "if 'tech_stack' in icp_filters:\n",
    "    tech_stack = icp_filters['tech_stack']\n",
    "    if isinstance(tech_stack, list):\n",
    "        for tech in tech_stack:\n",
    "            # Keep full tech names for exact matching\n",
    "            icp_tokens.append(str(tech).lower())\n",
    "    elif isinstance(tech_stack, str):\n",
    "        icp_tokens.append(tech_stack.lower())\n",
    " \n",
    "# Extract keyword tokens\n",
    "if 'keywords' in icp_filters:\n",
    "    keywords = icp_filters['keywords']\n",
    "    if isinstance(keywords, list):\n",
    "        for keyword in keywords:\n",
    "            icp_tokens.append(str(keyword).lower())\n",
    "    elif isinstance(keywords, str):\n",
    "        icp_tokens.append(keywords.lower())\n",
    "\n",
    "\n",
    "# Extract location tokens\n",
    "for field_name in ['location', 'locations']:\n",
    "    if field_name in icp_filters:\n",
    "        locations = icp_filters[field_name]\n",
    "        if isinstance(locations, list):\n",
    "            for location in locations:\n",
    "                tokens = str(location).lower().split()\n",
    "                icp_tokens.extend(tokens)\n",
    "        elif isinstance(locations, str):\n",
    "            tokens = locations.lower().split()\n",
    "            icp_tokens.extend(tokens)\n",
    "\n",
    "\n",
    "\n",
    "# Employee count\n",
    "if 'employee_count' in icp_filters:\n",
    "    emp_filter = icp_filters['employee_count']\n",
    "    if isinstance(emp_filter, dict):\n",
    "        icp_field_tokens['employee_count'] = emp_filter  # keep structured for later numeric match\n",
    "        if 'min' in emp_filter:\n",
    "            icp_tokens.append(f\"emp_min_{emp_filter['min']}\")\n",
    "        if 'max' in emp_filter:\n",
    "            icp_tokens.append(f\"emp_max_{emp_filter['max']}\")\n",
    "\n",
    "# Founded after\n",
    "if 'founded_after' in icp_filters:\n",
    "    founded_year = icp_filters['founded_after']\n",
    "    icp_field_tokens['founded_after'] = founded_year  # keep structured\n",
    "    icp_tokens.append(f\"founded_after_{founded_year}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean up tokens\n",
    "icp_tokens = list(set(token.strip() for token in icp_tokens if token.strip()))\n",
    "\n",
    "print(f\"✅ Extracted ICP tokens: {icp_tokens}\")\n",
    "print(f\"✅ Field-specific tokens: {icp_field_tokens}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 6. Score companies using util.cos_sim() ---\n",
    "scored_results_util = []\n",
    "icp_tensor = torch.tensor([icp_embedding], dtype=torch.float32)  # Convert ICP to tensor\n",
    "\n",
    "for point in search_result.points:\n",
    "    payload = point.payload\n",
    "    company_id = payload.get(\"company_id\")\n",
    "    company_vector = point.vector\n",
    "\n",
    "    # Calculate cosine similarity using util.cos_sim()\n",
    "    company_tensor = torch.tensor([company_vector], dtype=torch.float32)\n",
    "    raw_similarity = util.cos_sim(icp_tensor, company_tensor)[0][0].item()\n",
    "    vector_similarity_score = ((raw_similarity + 1) / 2) * 100  # Normalize to 0–100\n",
    "\n",
    "    try:\n",
    "        if isinstance(company_id, str):\n",
    "            company_id = ObjectId(company_id)\n",
    "    except Exception as e:\n",
    "        print(f'{company_id} has invalid ObjectId: {e}')\n",
    "        continue \n",
    "\n",
    "    company = discovered_companies.find_one({\"_id\": company_id})\n",
    "    if not company:\n",
    "        continue\n",
    "\n",
    "    # Apply rule-based scoring\n",
    "    rule_score_total, breakdown = rule_score(company, icp_tokens, icp_field_tokens)\n",
    "    print(\"\\n\",breakdown)\n",
    "    print(rule_score)\n",
    "    # Final score weights\n",
    "    vector_weight = 0.4\n",
    "    rule_weight = 0.6\n",
    "\n",
    "    final_score = round(\n",
    "        (vector_similarity_score * vector_weight) + (rule_score_total * rule_weight),\n",
    "        2\n",
    "    )\n",
    "\n",
    "    breakdown[\"vector_similarity\"] = round(vector_similarity_score, 2)\n",
    "\n",
    "    scored_doc = {\n",
    "        \"company_id\": company_id,\n",
    "        \"icp_id\": icp_id,\n",
    "        \"icp_version\": icp_version,\n",
    "        \"final_score\": final_score,\n",
    "        \"breakdown\": breakdown,\n",
    "        \"weights\": WEIGHTS,\n",
    "        \"last_scored\": datetime.now(timezone.utc),\n",
    "        \"method\": \"util_cos_sim\"\n",
    "    }\n",
    "\n",
    "    scored_companies.update_one(\n",
    "        {\"company_id\": company_id, \"icp_id\": icp_id, \"method\": \"util_cos_sim\"},\n",
    "        {\"$set\": scored_doc},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "    scored_results_util.append((company.get(\"domain\", \"unknown\"), final_score))\n",
    "\n",
    "# --- 7. Print Top Results ---\n",
    "print(f\"📈 Total companies scored: {len(scored_results_util)}\")\n",
    "\n",
    "sorted_results = sorted(scored_results_util, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n🏆 TOP RESULTS (Weighted Average Method):\")\n",
    "for i, (domain, score) in enumerate(sorted_results[:10], 1):\n",
    "    print(f\"{i:2d}. {domain:20s} → {score:6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bde6089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'locations': ['usa', 'canada'],\n",
       " 'tech_stack': ['AWS', 'python'],\n",
       " 'keywords': ['ai, ml'],\n",
       " 'designations': ['hr'],\n",
       " 'department': ['tect', 'aq', 'it'],\n",
       " 'industry': ['software', 'ai'],\n",
       " 'employee_count': {'min': 1, 'max': 500},\n",
       " 'founded_after': 2019}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icp_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1aaab",
   "metadata": {},
   "source": [
    "## test code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a293b3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total points returned from Qdrant: 5\n",
      "✅ Extracted ICP tokens: ['usa', 'ai', 'software', 'ai, ml', 'canada', 'python', 'aws']\n",
      "✅ Field-specific tokens: {'industry': ['software', 'ai'], 'location': ['usa', 'canada'], 'tech_stack': ['aws', 'python'], 'keywords': ['ai, ml'], 'employee_count': {'min': 1, 'max': 500}, 'founded_after': 2019}\n",
      "📈 Total companies scored: 5\n",
      "\n",
      "🏆 TOP RESULTS (Weighted Average Method):\n",
      " 1. flowcast.ai          →  62.94%\n",
      " 2. ssynthai.io          →  62.94%\n",
      " 3. ddocstream.com       →  59.94%\n",
      " 4. neogencloud.com      →  59.94%\n",
      " 5. retico.ai            →  59.94%\n"
     ]
    }
   ],
   "source": [
    "WEIGHTS = {\n",
    "    \"industry\": 20,\n",
    "    \"employee_count\": 15,\n",
    "    \"location\": 15,\n",
    "    \"tech_stack\": 25,\n",
    "    \"keywords\": 10,\n",
    "    \"founded_after\": 5,\n",
    "    \"github_signal\": 10\n",
    "}\n",
    " \n",
    "# --- 1. Get latest active ICP ---\n",
    "latest_icp = list(icp_profiles.aggregate(pipeline))\n",
    " \n",
    "if not latest_icp:\n",
    "    print(\"❌ No active ICP found.\")\n",
    "    exit()\n",
    "icp = latest_icp[0]\n",
    "icp_id = str(icp[\"_id\"])\n",
    "icp_version = icp.get(\"version\", 1)\n",
    " \n",
    "# --- 2. Get ICP vector from Qdrant ---\n",
    " \n",
    "icp_vector_resp = qdrant.retrieve(\n",
    "    collection_name=\"icp_vectors\",\n",
    "    ids= [str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"icp:{icp_id}\"))],\n",
    "    with_vectors=True\n",
    ")\n",
    " \n",
    "if not icp_vector_resp or not icp_vector_resp[0].vector:\n",
    "    print(\"❌ ICP vector not found.\")\n",
    "    exit()\n",
    " \n",
    "icp_embedding = icp_vector_resp[0].vector\n",
    " \n",
    "# --- 3. Get all company vectors from Qdrant ---\n",
    "search_result = qdrant.query_points(\n",
    "    collection_name=\"company_vectors\",\n",
    "    query=icp_embedding,\n",
    "    limit=10000,\n",
    "    with_payload=True,\n",
    "    with_vectors=True\n",
    ")\n",
    "print(f\"Total points returned from Qdrant: {len(search_result.points)}\")\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# --- 5. Tokenize ICP filters ---\n",
    "icp_filters = icp.get('filters', {})\n",
    "icp_tokens = []\n",
    "icp_field_tokens = {}  # Track tokens by field for better debugging\n",
    " \n",
    "# Extract industry tokens\n",
    "for field_name in ['industry', 'industries']:\n",
    "    if field_name in icp_filters:\n",
    "        industries = icp_filters[field_name]\n",
    "        field_tokens = []\n",
    "        if isinstance(industries, list):\n",
    "            for industry in industries:\n",
    "                tokens = str(industry).lower().split()\n",
    "                field_tokens.extend(tokens)\n",
    "                icp_tokens.extend(tokens)\n",
    "        elif isinstance(industries, str):\n",
    "            tokens = industries.lower().split()\n",
    "            field_tokens.extend(tokens)\n",
    "            icp_tokens.extend(tokens)\n",
    "        icp_field_tokens['industry'] = field_tokens\n",
    " \n",
    "# Extract location tokens\n",
    "for field_name in ['location', 'locations']:\n",
    "    if field_name in icp_filters:\n",
    "        locations = icp_filters[field_name]\n",
    "        field_tokens = []\n",
    "        if isinstance(locations, list):\n",
    "            for location in locations:\n",
    "                tokens = str(location).lower().split()\n",
    "                field_tokens.extend(tokens)\n",
    "                icp_tokens.extend(tokens)\n",
    "        elif isinstance(locations, str):\n",
    "            tokens = locations.lower().split()\n",
    "            field_tokens.extend(tokens)\n",
    "            icp_tokens.extend(tokens)\n",
    "        icp_field_tokens['location'] = field_tokens\n",
    " \n",
    "# Extract tech stack tokens\n",
    "if 'tech_stack' in icp_filters:\n",
    "    tech_stack = icp_filters['tech_stack']\n",
    "    field_tokens = []\n",
    "    if isinstance(tech_stack, list):\n",
    "        for tech in tech_stack:\n",
    "            # Keep full tech names for exact matching\n",
    "            token = str(tech).lower()\n",
    "            field_tokens.append(token)\n",
    "            icp_tokens.append(token)\n",
    "    elif isinstance(tech_stack, str):\n",
    "        token = tech_stack.lower()\n",
    "        field_tokens.append(token)\n",
    "        icp_tokens.append(token)\n",
    "    icp_field_tokens['tech_stack'] = field_tokens\n",
    " \n",
    "# Extract keyword tokens\n",
    "if 'keywords' in icp_filters:\n",
    "    keywords = icp_filters['keywords']\n",
    "    field_tokens = []\n",
    "    if isinstance(keywords, list):\n",
    "        for keyword in keywords:\n",
    "            token = str(keyword).lower()\n",
    "            field_tokens.append(token)\n",
    "            icp_tokens.append(token)\n",
    "    elif isinstance(keywords, str):\n",
    "        token = keywords.lower()\n",
    "        field_tokens.append(token)\n",
    "        icp_tokens.append(token)\n",
    "    icp_field_tokens['keywords'] = field_tokens\n",
    " \n",
    "# Extract employee count range (for matching logic)\n",
    "if 'employee_count' in icp_filters:\n",
    "    emp_filter = icp_filters['employee_count']\n",
    "    icp_field_tokens['employee_count'] = emp_filter  # Store the range dict\n",
    " \n",
    "# Extract founded year (for matching logic)\n",
    "# if 'founded_after' in icp:\n",
    "#     icp_field_tokens['founded_after'] = icp['founded_after']\n",
    "if 'founded_after' in icp_filters:\n",
    "    icp_field_tokens['founded_after'] = icp_filters['founded_after']\n",
    " \n",
    "# Clean up tokens\n",
    "icp_tokens = [token.strip() for token in icp_tokens if token.strip()]\n",
    "icp_tokens = list(set(icp_tokens))  # Remove duplicates\n",
    " \n",
    "print(f\"✅ Extracted ICP tokens: {icp_tokens}\")\n",
    "print(f\"✅ Field-specific tokens: {icp_field_tokens}\")\n",
    " \n",
    " \n",
    "\n",
    "# icp_tokens = list(set([token for token in icp_tokens if token.strip()]))\n",
    "\n",
    "\n",
    "# icp_founded_after = icp.get(\"filters\", {}).get(\"founded_after\", 2015)\n",
    "\n",
    "# --- 4. Scoring Rule Function () ---\n",
    "def rule_score(company_doc, icp_tokens, icp_field_tokens):\n",
    "    rule_score = 0\n",
    "    breakdown = {k: 0 for k in WEIGHTS}\n",
    "    def safe_list(field):\n",
    "        value = company_doc.get(field, [])\n",
    "        if isinstance(value, str): return [value]\n",
    "        if isinstance(value, list): return value\n",
    "        return []\n",
    "    \n",
    "    # 1. INDUSTRY MATCHING\n",
    "    if icp_field_tokens and 'industry' in icp_field_tokens:\n",
    "        industry_tokens = icp_field_tokens['industry']\n",
    "        company_industries = safe_list(\"industries\")\n",
    "        if any(token in str(v).lower() for v in company_industries for token in industry_tokens):\n",
    "            rule_score += WEIGHTS[\"industry\"]\n",
    "            breakdown[\"industry\"] = WEIGHTS[\"industry\"]\n",
    "    \n",
    "    # 2. LOCATION MATCHING  \n",
    "    if icp_field_tokens and 'location' in icp_field_tokens:\n",
    "        location_tokens = icp_field_tokens['location']\n",
    "        company_location_values = list(company_doc.get(\"hq_location\", {}).values())\n",
    "        if any(token in str(v).lower() for v in company_location_values for token in location_tokens):\n",
    "            rule_score += WEIGHTS[\"location\"]\n",
    "            breakdown[\"location\"] = WEIGHTS[\"location\"]\n",
    "    \n",
    "    # 3. TECH STACK MATCHING\n",
    "    if icp_field_tokens and 'tech_stack' in icp_field_tokens:\n",
    "        tech_tokens = icp_field_tokens['tech_stack']\n",
    "        company_tech = safe_list(\"tech_stack\")\n",
    "        if any(token in str(v).lower() for v in company_tech for token in tech_tokens):\n",
    "            rule_score += WEIGHTS[\"tech_stack\"]\n",
    "            breakdown[\"tech_stack\"] = WEIGHTS[\"tech_stack\"]\n",
    "    \n",
    "    # 4. KEYWORDS MATCHING\n",
    "    if icp_field_tokens and 'keywords' in icp_field_tokens:\n",
    "        keyword_tokens = icp_field_tokens['keywords']\n",
    "        text_fields = [\n",
    "            company_doc.get(\"description\", \"\"),\n",
    "            company_doc.get(\"name\", \"\"),\n",
    "            \" \".join(safe_list(\"industries\")),\n",
    "            \" \".join(safe_list(\"tech_stack\"))\n",
    "        ]\n",
    "        company_text = \" \".join(text_fields).lower()\n",
    "        if any(token in company_text for token in keyword_tokens):\n",
    "            rule_score += WEIGHTS[\"keywords\"]\n",
    "            breakdown[\"keywords\"] = WEIGHTS[\"keywords\"]\n",
    "    # 5. EMPLOYEE COUNT MATCHING (DYNAMIC)\n",
    "    emp = company_doc.get(\"employee_count_estimate\", {})\n",
    "    emp_min = emp.get(\"min\", 0)\n",
    "    emp_max = emp.get(\"max\", float('inf'))\n",
    "    \n",
    "    if icp_field_tokens and 'employee_count' in icp_field_tokens:\n",
    "        icp_emp = icp_field_tokens['employee_count']\n",
    "        if isinstance(icp_emp, dict):\n",
    "            icp_min = icp_emp.get('min')\n",
    "            icp_max = icp_emp.get('max')\n",
    "            \n",
    "            # Check if company employee count falls within the ICP range\n",
    "            if not (emp_max< icp_min or emp_min > icp_max):\n",
    "                rule_score += WEIGHTS[\"employee_count\"]\n",
    "                breakdown[\"employee_count\"] = WEIGHTS[\"employee_count\"]\n",
    "    # 6. FOUNDED YEAR MATCHING (DYNAMIC)\n",
    "    company_founded_year = company_doc.get(\"founded_year\")  # FIXED field name\n",
    "\n",
    "    if icp_field_tokens and 'founded_after' in icp_field_tokens:\n",
    "        year_threshold = icp_field_tokens['founded_after']\n",
    "\n",
    "    if isinstance(company_founded_year, int) and company_founded_year >= year_threshold:\n",
    "        rule_score += WEIGHTS[\"founded_after\"]\n",
    "        breakdown[\"founded_after\"] = WEIGHTS[\"founded_after\"]\n",
    "\n",
    "\n",
    "\n",
    "    # 7. GITHUB SIGNAL MATCHING\n",
    "    urls = company_doc.get(\"source_urls\", [])\n",
    "    if any(\"github\" in url.lower() for url in urls):\n",
    "        rule_score += WEIGHTS[\"github_signal\"]\n",
    "        breakdown[\"github_signal\"] = WEIGHTS[\"github_signal\"]\n",
    "    return rule_score, breakdown\n",
    " \n",
    "\n",
    "\n",
    "# --- 6. Score companies using util.cos_sim() ---\n",
    "scored_results_util = []\n",
    "icp_tensor = torch.tensor([icp_embedding], dtype=torch.float32)  # Convert ICP to tensor\n",
    "\n",
    "for point in search_result.points:\n",
    "    payload = point.payload\n",
    "    company_id = payload.get(\"company_id\")\n",
    "    company_vector = point.vector\n",
    "\n",
    "    # Calculate cosine similarity using util.cos_sim()\n",
    "    company_tensor = torch.tensor([company_vector], dtype=torch.float32)\n",
    "    raw_similarity = util.cos_sim(icp_tensor, company_tensor)[0][0].item()\n",
    "    vector_similarity_score = ((raw_similarity + 1) / 2) * 100  # Normalize to 0–100\n",
    "\n",
    "    try:\n",
    "        if isinstance(company_id, str):\n",
    "            company_id = ObjectId(company_id)\n",
    "    except Exception as e:\n",
    "        print(f'{company_id} has invalid ObjectId: {e}')\n",
    "        continue \n",
    "\n",
    "\n",
    "# Fetch all companies from the collection\n",
    "for company in discovered_companies.find({}):\n",
    "    # Apply rule-based scoring\n",
    "    score, breakdown = rule_score(company, icp_tokens, icp_field_tokens)\n",
    "\n",
    "\n",
    "\n",
    "    # Final score weights\n",
    "    vector_weight = 0.4\n",
    "    rule_weight = 0.6\n",
    "\n",
    "    final_score = round(\n",
    "        (vector_similarity_score * vector_weight) + (score * rule_weight),\n",
    "        2\n",
    "    )\n",
    "\n",
    "    breakdown[\"vector_similarity\"] = round(vector_similarity_score, 2)\n",
    "\n",
    "    scored_doc = {\n",
    "        \"company_id\": company_id,\n",
    "        \"icp_id\": icp_id,\n",
    "        \"icp_version\": icp_version,\n",
    "        \"final_score\": final_score,\n",
    "        \"breakdown\": breakdown,\n",
    "        \"weights\": WEIGHTS,\n",
    "        \"last_scored\": datetime.now(timezone.utc),\n",
    "        \"method\": \"util_cos_sim\"\n",
    "    }\n",
    "\n",
    "    scored_companies.update_one(\n",
    "        {\"company_id\": company_id, \"icp_id\": icp_id, \"method\": \"util_cos_sim\"},\n",
    "        {\"$set\": scored_doc},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "    scored_results_util.append((company.get(\"domain\", \"unknown\"), final_score))\n",
    "\n",
    "# --- 7. Print Top Results ---\n",
    "print(f\"📈 Total companies scored: {len(scored_results_util)}\")\n",
    "\n",
    "sorted_results = sorted(scored_results_util, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n🏆 TOP RESULTS (Weighted Average Method):\")\n",
    "for i, (domain, score) in enumerate(sorted_results[:10], 1):\n",
    "    print(f\"{i:2d}. {domain:20s} → {score:6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df9d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'industry': 0, 'employee_count': 15, 'location': 0, 'tech_stack': 25, 'keywords': 0, 'founded_after': 5, 'github_signal': 0}\n",
      "{'industry': 0, 'employee_count': 15, 'location': 0, 'tech_stack': 25, 'keywords': 0, 'founded_after': 5, 'github_signal': 0}\n",
      "{'industry': 0, 'employee_count': 15, 'location': 0, 'tech_stack': 0, 'keywords': 0, 'founded_after': 0, 'github_signal': 0}\n",
      "{'industry': 0, 'employee_count': 15, 'location': 0, 'tech_stack': 0, 'keywords': 0, 'founded_after': 5, 'github_signal': 0}\n",
      "{'industry': 0, 'employee_count': 15, 'location': 0, 'tech_stack': 25, 'keywords': 0, 'founded_after': 0, 'github_signal': 0}\n"
     ]
    }
   ],
   "source": [
    "for company in discovered_companies.find({}):\n",
    "    score, breakdown = rule_score(company_doc=company, icp_tokens=icp_tokens, icp_field_tokens=icp_field_tokens)\n",
    "    print(breakdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88239f47",
   "metadata": {},
   "source": [
    "#### Store to MongoDB :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "914773d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scored_companies = db['scored_companies']\n",
    "\n",
    "# Call this after scoring each company\n",
    "def store_scored_company(company, score_tuple, icp_id, icp_version=1):\n",
    "    score, breakdown, similarity = score_tuple\n",
    "\n",
    "    company_oid = ObjectId(company[\"_id\"])\n",
    "    icp_oid = ObjectId(icp_id)\n",
    "\n",
    "    doc = {\n",
    "        \"company_id\": company_oid,\n",
    "        \"icp_id\": icp_oid,\n",
    "        \"score\": float(score),\n",
    "        \"breakdown\": {\n",
    "            **breakdown,\n",
    "            \"vector_similarity\": float(similarity)\n",
    "        },\n",
    "        \"last_scored\": datetime.now(timezone.utc),\n",
    "        \"icp_version\": icp_version\n",
    "    }\n",
    "\n",
    "    # 🛠 Upsert safely by matching ObjectId fields\n",
    "    scored_companies.update_one(\n",
    "        {\"company_id\": company_oid, \"icp_id\": icp_oid},\n",
    "        {\"$set\": doc},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "# Store in a different collection or add method flag\n",
    "    scored_companies.update_one(\n",
    "        {\"company_id\": company_id, \"icp_id\": icp_id, \"method\": \"util_cos_sim\"},\n",
    "        {\"$set\": scored_doc},\n",
    "        upsert=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15c31277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation (similarity vs. final score): 0.933\n"
     ]
    }
   ],
   "source": [
    "final_scores = []\n",
    "similarities = []\n",
    "\n",
    "for doc in scored_companies.find():\n",
    "    similarities.append(doc[\"breakdown\"][\"vector_similarity\"])\n",
    "    final_scores.append(doc[\"final_score\"])\n",
    "\n",
    "corr, _ = pearsonr(similarities, final_scores)\n",
    "print(f\"Pearson correlation (similarity vs. final score): {round(corr, 3)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0a2ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation (similarity vs. final score): 0.926\n"
     ]
    }
   ],
   "source": [
    "final_scores = []\n",
    "similarities = []\n",
    "\n",
    "for doc in scored_companies.find():\n",
    "    similarities.append(doc[\"breakdown\"][\"vector_similarity\"])\n",
    "    final_scores.append(doc[\"final_score\"])\n",
    "    \n",
    "corr, _ = spearmanr(similarities, final_scores)\n",
    "print(f\"Spearman correlation (similarity vs. final score): {round(corr, 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
