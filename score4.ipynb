{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffa75dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, Distance, VectorParams, ScoredPoint, Filter, FieldCondition, MatchValue\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "from bson import ObjectId\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe83496",
   "metadata": {},
   "source": [
    "#### Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "642a1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# mongo conection\n",
    "\n",
    "mongo_uri = \"mongodb://localhost:27017/\" #compass\n",
    "mongo_client = MongoClient(mongo_uri)\n",
    "db = mongo_client['pipelineos']\n",
    "\n",
    "# loading the mongodb into variables:\n",
    "\n",
    "icp_profiles = db[\"icp_profiles\"]\n",
    "discovered_companies = db[\"discovered_companies\"]\n",
    "scored_companies = db['scored_companies']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0b9f3",
   "metadata": {},
   "source": [
    "#### Qdrant connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b69b0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qdrant connection via https\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "qdrant = QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333)\n",
    "#using qdrants https method as used 6333 port and gRPC is :\n",
    "# qdrant = QdrantClient(\n",
    "#     host=\"localhost\", \n",
    "#     port=6334, \n",
    "#     grpc=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017c1bb",
   "metadata": {},
   "source": [
    "#### Run the Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc600910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quadrant setup via docker:\n",
    "# run this in ubuntu terminal:\n",
    "\n",
    "# docker pull qdrant/qdrant\n",
    "# docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25c3ef",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "214b1c8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /api/models/sentence-transformers/all-MiniLM-L12-v2/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7947f5d7ab60>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 3c920a45-4310-44fb-b192-cd63590f71e2)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/urllib3/connection.py:753\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7947f5d7ab60>: Failed to resolve 'huggingface.co' ([Errno -3] Temporary failure in name resolution)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/sentence-transformers/all-MiniLM-L12-v2/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7947f5d7ab60>: Failed to resolve 'huggingface.co' ([Errno -3] Temporary failure in name resolution)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# # vectorize setup:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') #384-dim vector\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model = SentenceTransformer(\"BAAI/bge-small-en\")  # 384 D\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall-MiniLM-L12-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 384 D\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# model = SentenceTransformer(\"all-mpnet-base-v2\") # 768 d\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# model = SentenceTransformer(\"all-distilroberta-v1\") # 768\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\") # 384 d\u001b[39;00m\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:327\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    309\u001b[0m has_modules \u001b[38;5;241m=\u001b[39m is_sentence_transformer_model(\n\u001b[1;32m    310\u001b[0m     model_name_or_path,\n\u001b[1;32m    311\u001b[0m     token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    315\u001b[0m )\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    317\u001b[0m     has_modules\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_type(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    326\u001b[0m ):\n\u001b[0;32m--> 327\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    340\u001b[0m         model_name_or_path,\n\u001b[1;32m    341\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m         has_modules\u001b[38;5;241m=\u001b[39mhas_modules,\n\u001b[1;32m    350\u001b[0m     )\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:2254\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   2249\u001b[0m         module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(local_path)\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2252\u001b[0m     \u001b[38;5;66;03m# Newer modules that support the new loading method are loaded with the new style\u001b[39;00m\n\u001b[1;32m   2253\u001b[0m     \u001b[38;5;66;03m# i.e. with many keyword arguments that can optionally be used by the modules\u001b[39;00m\n\u001b[0;32m-> 2254\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Loading-specific keyword arguments\u001b[39;49;00m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Module-specific keyword arguments\u001b[39;49;00m\n\u001b[1;32m   2263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2270\u001b[0m modules[module_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m   2271\u001b[0m module_kwargs[module_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m module_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:541\u001b[0m, in \u001b[0;36mTransformer.load\u001b[0;34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    527\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m    528\u001b[0m     init_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_init_kwargs(\n\u001b[1;32m    529\u001b[0m         model_name_or_path\u001b[38;5;241m=\u001b[39mmodel_name_or_path,\n\u001b[1;32m    530\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m         backend\u001b[38;5;241m=\u001b[39mbackend,\n\u001b[1;32m    540\u001b[0m     )\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:92\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[1;32m     91\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name_or_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenizer_name_or_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# No max_seq_length set. Try to infer from model\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1114\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1111\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1112\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1113\u001b[0m         )\n\u001b[0;32m-> 1114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1956\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1952\u001b[0m                         vocab_files[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1953\u001b[0m                             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1954\u001b[0m                         )\n\u001b[1;32m   1955\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1956\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m template \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlist_repo_templates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1962\u001b[0m                     vocab_files[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1964\u001b[0m \u001b[38;5;66;03m# Get files from url, cache, or disk depending on the case\u001b[39;00m\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:167\u001b[0m, in \u001b[0;36mlist_repo_templates\u001b[0;34m(repo_id, local_files_only, revision, cache_dir)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    168\u001b[0m             entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mremoveprefix(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m list_repo_tree(\n\u001b[1;32m    170\u001b[0m                 repo_id\u001b[38;5;241m=\u001b[39mrepo_id, revision\u001b[38;5;241m=\u001b[39mrevision, path_in_repo\u001b[38;5;241m=\u001b[39mCHAT_TEMPLATE_DIR, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    171\u001b[0m             )\n\u001b[1;32m    172\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m         ]\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:167\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    168\u001b[0m             entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mremoveprefix(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m list_repo_tree(\n\u001b[1;32m    170\u001b[0m                 repo_id\u001b[38;5;241m=\u001b[39mrepo_id, revision\u001b[38;5;241m=\u001b[39mrevision, path_in_repo\u001b[38;5;241m=\u001b[39mCHAT_TEMPLATE_DIR, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    171\u001b[0m             )\n\u001b[1;32m    172\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m         ]\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/huggingface_hub/hf_api.py:3171\u001b[0m, in \u001b[0;36mHfApi.list_repo_tree\u001b[0;34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[0m\n\u001b[1;32m   3169\u001b[0m encoded_path_in_repo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m quote(path_in_repo, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m path_in_repo \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3170\u001b[0m tree_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mencoded_path_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path_info \u001b[38;5;129;01min\u001b[39;00m paginate(path\u001b[38;5;241m=\u001b[39mtree_url, headers\u001b[38;5;241m=\u001b[39mheaders, params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursive\u001b[39m\u001b[38;5;124m\"\u001b[39m: recursive, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m\"\u001b[39m: expand}):\n\u001b[1;32m   3172\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (RepoFile(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpath_info) \u001b[38;5;28;01mif\u001b[39;00m path_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m RepoFolder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpath_info))\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_pagination.py:36\u001b[0m, in \u001b[0;36mpaginate\u001b[0;34m(path, params, headers)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fetch a list of models/datasets/spaces and paginate through results.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mThis is using the same \"Link\" header format as GitHub.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m- https://docs.github.com/en/rest/guides/traversing-with-pagination#link-header\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m session \u001b[38;5;241m=\u001b[39m get_session()\n\u001b[0;32m---> 36\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/pipeline OS/.venv/lib/python3.10/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /api/models/sentence-transformers/all-MiniLM-L12-v2/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7947f5d7ab60>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 3c920a45-4310-44fb-b192-cd63590f71e2)')"
     ]
    }
   ],
   "source": [
    "# # vectorize setup:\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') #384-dim vector\n",
    "# model = SentenceTransformer(\"BAAI/bge-small-en\")  # 384 D\n",
    "model = SentenceTransformer(\"all-MiniLM-L12-v2\") # 384 D\n",
    "# model = SentenceTransformer(\"all-mpnet-base-v2\") # 768 d\n",
    "# model = SentenceTransformer(\"all-distilroberta-v1\") # 768\n",
    "# model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\") # 384 d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81305cee",
   "metadata": {},
   "source": [
    "#### To fetch the latest ICP from the MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcf1e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest active ICP:\n",
      "{'_id': ObjectId('689584be825c9ee6d4573435'), 'name': 'Flowcast AI solutions', 'filters': {'locations': [], 'tech_stack': ['ai', 'AWS', 'LangChain', 'python'], 'keywords': ['ai', 'ml', 'gen'], 'designations': ['ceo', 'er'], 'department': ['Engineering', 'Product'], 'industry': ['FinTech', 'finance'], 'employee_count': {'min': 30, 'max': 20}, 'founded_after': 2019}, 'tags': ['tect'], 'active': True, 'created_at': datetime.datetime(2025, 8, 8, 5, 1, 50, 228000), 'updated_at': datetime.datetime(2025, 8, 8, 5, 1, 50, 228000), 'effective_time': datetime.datetime(2025, 8, 8, 5, 1, 50, 228000)}\n"
     ]
    }
   ],
   "source": [
    "# Aggregation pipeline to get the latest active ICP\n",
    "pipeline = [\n",
    "    {\"$match\": {\"active\": True}},\n",
    "    {\"$addFields\": {\n",
    "        \"effective_time\": {\n",
    "            \"$cond\": {\n",
    "                \"if\": {\"$gt\": [\"$updated_at\", \"$created_at\"]},\n",
    "                \"then\": \"$updated_at\",\n",
    "                \"else\": \"$created_at\"\n",
    "            }\n",
    "        }\n",
    "    }},\n",
    "    {\"$sort\": {\"effective_time\": -1}},\n",
    "    {\"$limit\": 1}\n",
    "]\n",
    "\n",
    "latest_icp = list(icp_profiles.aggregate(pipeline))\n",
    "if latest_icp:\n",
    "    latest_icp = latest_icp[0]\n",
    "    print(\"Latest active ICP:\")\n",
    "    print(latest_icp)\n",
    "else:\n",
    "    print(\"No active ICP found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a92a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('689584be825c9ee6d4573435'),\n",
       " 'name': 'Flowcast AI solutions',\n",
       " 'filters': {'locations': [],\n",
       "  'tech_stack': ['ai', 'AWS', 'LangChain', 'python'],\n",
       "  'keywords': ['ai', 'ml', 'gen'],\n",
       "  'designations': ['ceo', 'er'],\n",
       "  'department': ['Engineering', 'Product'],\n",
       "  'industry': ['FinTech', 'finance'],\n",
       "  'employee_count': {'min': 30, 'max': 20},\n",
       "  'founded_after': 2019},\n",
       " 'tags': ['tect'],\n",
       " 'active': True,\n",
       " 'created_at': datetime.datetime(2025, 8, 8, 5, 1, 50, 228000),\n",
       " 'updated_at': datetime.datetime(2025, 8, 8, 5, 1, 50, 228000),\n",
       " 'effective_time': datetime.datetime(2025, 8, 8, 5, 1, 50, 228000)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_icp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04317eb",
   "metadata": {},
   "source": [
    "#### Delta Ware Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94e718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total <company, ICP> pairs to score: 5\n"
     ]
    }
   ],
   "source": [
    "latest_icp = list(icp_profiles.aggregate(pipeline))\n",
    "companies_to_score = []\n",
    "\n",
    "for icp in latest_icp:\n",
    "    icp_id = str(icp[\"_id\"])\n",
    "    icp_version = icp.get(\"version\", 1)\n",
    "\n",
    "    for company in discovered_companies.find({}):\n",
    "        company_id = company[\"_id\"]\n",
    "        last_scraped = company[\"last_scraped\"]\n",
    "\n",
    "\n",
    "        # Look up the scored company metadata from separate collection\n",
    "        score_doc = scored_companies.find_one({\n",
    "            \"company_id\": company_id,\n",
    "            \"icp_id\": icp_id\n",
    "        })\n",
    "\n",
    "        # CASE A: Never scored for this ICP\n",
    "        if not score_doc:\n",
    "            companies_to_score.append({\"company\": company, \"icp\": icp})\n",
    "            continue\n",
    "\n",
    "        # CASE B: Rescraped after last scoring\n",
    "        last_scored = score_doc.get(\"last_scored\")\n",
    "        if last_scored is None or last_scored > last_scraped: # logic changed just to check, reverse it once the task is done\n",
    "            companies_to_score.append({\"company\": company, \"icp\": icp})\n",
    "            continue\n",
    "\n",
    "        # CASE C: ICP version has changed\n",
    "        scored_version = score_doc.get(\"icp_version\")\n",
    "        if scored_version != icp_version:\n",
    "            companies_to_score.append({\"company\": company, \"icp\": icp})\n",
    "            continue\n",
    "\n",
    "print(f\"Total <company, ICP> pairs to score: {len(companies_to_score)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc369b8",
   "metadata": {},
   "source": [
    "### Collections Creation in Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === First: Create collection if it doesn't exist ===\n",
    "\n",
    "\n",
    "# if qdrant.collection_exists(\"icp_vectors\"):\n",
    "#     qdrant.delete_collection(\"icp_vectors\")\n",
    "\n",
    "# ICP_Vectors\n",
    "\n",
    "if not qdrant.collection_exists(\"icp_vectors\"):\n",
    "    qdrant.create_collection(\n",
    "        collection_name=\"icp_vectors\",\n",
    "        vectors_config=VectorParams(size=384, distance=Distance.COSINE)  # 384 for MiniLM\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# if qdrant.collection_exists(\"company_vectors\"):\n",
    "#     qdrant.delete_collection(\"company_vectors\")\n",
    "\n",
    "# Company_Vectors\n",
    "\n",
    "if not qdrant.collection_exists(\"company_vectors\"):\n",
    "    qdrant.create_collection(\n",
    "        collection_name=\"company_vectors\",\n",
    "        vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506967b",
   "metadata": {},
   "source": [
    "#### Company Vectorization and upsert with filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77effc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# import numpy as np\n",
    "# import uuid\n",
    "# from pymongo import UpdateOne\n",
    "# from qdrant_client.models import PointStruct\n",
    "# from bson import ObjectId\n",
    "\n",
    "# def normalize_list(value):\n",
    "#     if isinstance(value, str):\n",
    "#         return [value.lower()]\n",
    "#     if isinstance(value, list):\n",
    "#         return list({v.strip().lower() for v in value if isinstance(v, str)})\n",
    "#     return []\n",
    "\n",
    "# def prepare_vector_text(company):\n",
    "#     parts = []\n",
    "#     parts += normalize_list(company.get(\"industries\"))\n",
    "#     parts += normalize_list(company.get(\"tech_stack\"))\n",
    "\n",
    "#     country = company.get(\"hq_location\", {}).get(\"country\", \"\").lower()\n",
    "#     if country:\n",
    "#         parts.append(country)\n",
    "\n",
    "#     return \" \".join(parts).strip()\n",
    "\n",
    "# points = []\n",
    "# bulk_updates = []\n",
    "\n",
    "# for pair in companies_to_score:\n",
    "#     company = pair[\"company\"]\n",
    "#     icp = pair[\"icp\"]\n",
    "\n",
    "#     domain = company.get(\"domain\")\n",
    "#     if not domain:\n",
    "#         continue\n",
    "\n",
    "#     # changed_at = company.get(\"changed_at\")\n",
    "#     # last_vectorized = company.get(\"last_vectorized\")\n",
    "\n",
    "#     # if changed_at and last_vectorized:\n",
    "#     #     if changed_at < last_vectorized:\n",
    "#     #         print(f\"Skipping {domain}: No changes since last vectorization.\")\n",
    "#     #         print(f\"[DEBUG] {domain} | changed_at: {changed_at}, last_vectorized: {last_vectorized}\")\n",
    "\n",
    "#     #         continue\n",
    "\n",
    "#     vector_text = prepare_vector_text(company)\n",
    "#     if not vector_text:\n",
    "#         print(f\"Skipping {domain}: No vector text.\")\n",
    "#         continue\n",
    "\n",
    "#     try:\n",
    "#         embedding = model.encode(vector_text)\n",
    "#         if embedding is None or not isinstance(embedding, (list, np.ndarray)) or np.all(np.array(embedding) == 0):\n",
    "#             print(f\"Skipping {domain}: Invalid embedding.\")\n",
    "#             continue\n",
    "\n",
    "#         embedding = np.array(embedding, dtype=np.float32).tolist()\n",
    "#         point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"company:{domain}\"))\n",
    "\n",
    "#         point = PointStruct(\n",
    "#             id=point_id,\n",
    "#             vector=embedding,\n",
    "#             payload={\n",
    "#                 \"company_id\": str(company.get(\"_id\")),\n",
    "#                 \"icp_id\": str(company.get(\"icp_id\", \"\")),\n",
    "#                 \"industry\": normalize_list(company.get(\"industries\")),\n",
    "#                 \"location\": company.get(\"hq_location\", {}).get(\"country\", \"\").lower(),\n",
    "#                 \"tech_stack\": normalize_list(company.get(\"tech_stack\"))\n",
    "#             }\n",
    "#         )\n",
    "#         points.append(point)\n",
    "\n",
    "#         bulk_updates.append(UpdateOne(\n",
    "#             {\"_id\": company[\"_id\"]},\n",
    "#             {\"$set\": {\"last_vectorized\": datetime.datetime.now(datetime.timezone.utc)}}\n",
    "#         ))\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error vectorizing {domain}: {e}\")\n",
    "\n",
    "# if points:\n",
    "#     qdrant.upsert(collection_name=\"company_vectors\", points=points)\n",
    "#     print(f\"Upserted {len(points)} companies to Qdrant.\")\n",
    "# else:\n",
    "#     print(\"No valid company vectors to upsert.\")\n",
    "\n",
    "# if bulk_updates:\n",
    "#     db[\"discovered_companies\"].bulk_write(bulk_updates)\n",
    "#     print(f\"Updated last_vectorized for {len(bulk_updates)} companies.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3133b59",
   "metadata": {},
   "source": [
    "#### ICP Vectorization and upsert with filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# from uuid import uuid5, NAMESPACE_DNS\n",
    "# import numpy as np\n",
    "# from qdrant_client.models import PointStruct\n",
    "\n",
    "# # === Helper: Normalize and join values ===\n",
    "# def normalize_list(value):\n",
    "#     if isinstance(value, str):\n",
    "#         return [value.lower()]\n",
    "#     if isinstance(value, list):\n",
    "#         return list({v.strip().lower() for v in value if isinstance(v, str)})\n",
    "#     return []\n",
    "\n",
    "# # === Load active ICPs ===\n",
    "# latest_icp = list(icp_profiles.aggregate(pipeline))\n",
    "# icp_points = []\n",
    "\n",
    "# for icp in latest_icp:\n",
    "#     icp_id = str(icp[\"_id\"])\n",
    "#     filters = icp.get(\"filters\", {})\n",
    "#     updated_at = icp.get(\"updated_at\", datetime.min)\n",
    "#     last_vectorized = icp.get(\"last_vectorized\")  # May be None\n",
    "\n",
    "#     # === Skip if already vectorized and not updated ===\n",
    "#     # if last_vectorized:\n",
    "#     #     if updated_at < last_vectorized:\n",
    "#     #         print(f\"Skipping ICP {icp_id}: No changes since last vectorization.\")\n",
    "#     #         continue\n",
    "\n",
    "#     # === Normalize and build vector text ===\n",
    "#     text_parts = []\n",
    "\n",
    "#     industries = normalize_list(filters.get(\"industries\", []))\n",
    "#     locations = normalize_list(filters.get(\"locations\", []))\n",
    "#     tech_stack = normalize_list(icp.get(\"tech_stack\", []))\n",
    "#     keywords = normalize_list(icp.get(\"keywords\", []))\n",
    "\n",
    "#     text_parts += industries + locations + tech_stack + keywords\n",
    "#     vector_text = \" \".join(text_parts).strip()\n",
    "\n",
    "#     if not vector_text:\n",
    "#         continue\n",
    "\n",
    "#     try:\n",
    "#         embedding = model.encode(vector_text)\n",
    "#         if embedding is None or not isinstance(embedding, (list, np.ndarray)) or np.all(np.array(embedding) == 0):\n",
    "#             continue\n",
    "\n",
    "#         embedding = np.array(embedding, dtype=np.float32).tolist()\n",
    "#         point_id = str(uuid5(NAMESPACE_DNS, f\"icp:{icp_id}\"))\n",
    "\n",
    "#         point = PointStruct(\n",
    "#             id=point_id,\n",
    "#             vector=embedding,\n",
    "#             payload={\n",
    "#                 \"icp_id\": icp_id,\n",
    "#                 \"industry\": industries,\n",
    "#                 \"location\": locations,\n",
    "#                 \"tech_stack\": tech_stack,\n",
    "#                 \"keywords\": keywords\n",
    "#             }\n",
    "#         )\n",
    "#         icp_points.append(point)\n",
    "\n",
    "#         # === Update last_vectorized timestamp in MongoDB ===\n",
    "#         # icp_profiles.update_one(\n",
    "#         #     {\"_id\": icp[\"_id\"]},\n",
    "#         #     {\"$set\": {\"last_vectorized\": datetime.utcnow()}}\n",
    "#         # )\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error encoding ICP {icp_id}: {e}\")\n",
    "\n",
    "# # === Final upsert to Qdrant ===\n",
    "# if icp_points:\n",
    "#     qdrant.upsert(collection_name=\"icp_vectors\", points=icp_points)\n",
    "#     print(f\"Upserted {len(icp_points)} ICP profiles to Qdrant.\")\n",
    "# else:\n",
    "#     print(\"No valid ICP vectors to upsert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f801d",
   "metadata": {},
   "source": [
    "#### without filter ICP and Company Vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc04458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 5 companies to Qdrant.\n"
     ]
    }
   ],
   "source": [
    "# === Helper: Normalize and join string lists ===\n",
    "def normalize_list(value):\n",
    "    if isinstance(value, str):\n",
    "        return [value.lower()]\n",
    "    if isinstance(value, list):\n",
    "        return list({v.strip().lower() for v in value if isinstance(v, str)})\n",
    "    return []\n",
    "\n",
    "# === Helper: Prepare vector text from normalized fields ===\n",
    "def prepare_vector_text(company):\n",
    "    parts = []\n",
    "\n",
    "    industries = normalize_list(company.get(\"industries\"))\n",
    "    tech_stack = normalize_list(company.get(\"tech_stack\"))\n",
    "    country = company.get(\"hq_location\", {}).get(\"country\", \"\").lower()\n",
    "\n",
    "    parts += industries\n",
    "    parts += tech_stack\n",
    "    if country:\n",
    "        parts.append(country)\n",
    "\n",
    "    return \" \".join(parts).strip()\n",
    "\n",
    "# === Vectorize and Upsert to Qdrant ===\n",
    "points = []\n",
    "for pair in companies_to_score:\n",
    "    company = pair[\"company\"]\n",
    "    icp = pair[\"icp\"]\n",
    "\n",
    "    domain = company.get(\"domain\")\n",
    "    if not domain:\n",
    "        continue\n",
    "\n",
    "    # === Check vector freshness ===\n",
    "    # changed_at = company.get(\"changed_at\")\n",
    "    # last_vectorized = company.get(\"last_vectorized\")\n",
    "\n",
    "    # if changed_at and last_vectorized:\n",
    "    #     try:\n",
    "    #         if isinstance(changed_at, str):\n",
    "    #             changed_at = datetime.datetime(changed_at)\n",
    "    #         if isinstance(last_vectorized, str):\n",
    "    #             last_vectorized = datetime.datetime(last_vectorized)\n",
    "\n",
    "    #         if changed_at <= last_vectorized:\n",
    "    #             print(f\"Skipping {domain}: No changes since last vectorization.\")\n",
    "    #             continue\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Date parse error for {domain}: {e}\")\n",
    "\n",
    "    vector_text = prepare_vector_text(company)\n",
    "    if not vector_text:\n",
    "        print(f\"Skipping {domain}: No vector text.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        embedding = model.encode(vector_text)\n",
    "        if embedding is None or not isinstance(embedding, (list, np.ndarray)) or np.all(np.array(embedding) == 0):\n",
    "            print(f\"Skipping {domain}: Invalid embedding.\")\n",
    "            continue\n",
    "\n",
    "        embedding = np.array(embedding, dtype=np.float32).tolist()\n",
    "        point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"company:{domain}\"))\n",
    "\n",
    "        point = PointStruct(\n",
    "            id=point_id,\n",
    "            vector=embedding,\n",
    "            payload={\n",
    "                \"company_id\": str(company.get(\"_id\")),\n",
    "                \"icp_id\": str(company.get(\"icp_id\", \"\")),\n",
    "                \"industry\": normalize_list(company.get(\"industries\")),\n",
    "                \"location\": company.get(\"hq_location\", {}).get(\"country\", \"\").lower(),\n",
    "                \"tech_stack\": normalize_list(company.get(\"tech_stack\"))\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "    except Exception as e:\n",
    "        print(f\"Error vectorizing {domain}: {e}\")\n",
    "\n",
    "# === Upsert all vectors into Qdrant ===\n",
    "if points:\n",
    "    qdrant.upsert(collection_name=\"company_vectors\", points=points)\n",
    "    print(f\"Upserted {len(points)} companies to Qdrant.\")\n",
    "else:\n",
    "    print(\"No valid company vectors to upsert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb351fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[689584be825c9ee6d4573435] vector_text: 'fintech finance ai aws python langchain' | filters: {'locations': [], 'tech_stack': ['ai', 'AWS', 'LangChain', 'python'], 'keywords': ['ai', 'ml', 'gen'], 'designations': ['ceo', 'er'], 'department': ['Engineering', 'Product'], 'industry': ['FinTech', 'finance'], 'employee_count': {'min': 30, 'max': 20}, 'founded_after': 2019}\n",
      "Upserted 1 ICP profiles to Qdrant.\n"
     ]
    }
   ],
   "source": [
    "# === Helper: Normalize and join values ===\n",
    "def normalize_list(value):\n",
    "    if isinstance(value, str):\n",
    "        return [value.lower()]\n",
    "    if isinstance(value, list):\n",
    "        return list({v.strip().lower() for v in value if isinstance(v, str)})\n",
    "    return []\n",
    "\n",
    "# === Load active ICPs ===\n",
    "latest_icp = list(icp_profiles.aggregate(pipeline))\n",
    "icp_points = []\n",
    "\n",
    "for icp in latest_icp:\n",
    "    icp_id = str(icp[\"_id\"])\n",
    "    filters = icp.get(\"filters\", {})\n",
    "    text_parts = []\n",
    "\n",
    "    # Normalize and extract fields\n",
    "    # FIXED\n",
    "    industry = normalize_list(filters.get(\"industry\", []))\n",
    "    locations = normalize_list(filters.get(\"locations\", []))\n",
    "    tech_stack = normalize_list(filters.get(\"tech_stack\", []))\n",
    "\n",
    "\n",
    "    text_parts += industry + locations + tech_stack\n",
    "    vector_text = \" \".join(text_parts).strip()\n",
    "\n",
    "    if not vector_text:\n",
    "        continue\n",
    "    print(f\"[{icp_id}] vector_text: '{vector_text}' | filters: {filters}\")\n",
    "\n",
    "    try:\n",
    "        embedding = model.encode(vector_text)\n",
    "        if embedding is None or not isinstance(embedding, (list, np.ndarray)) or np.all(np.array(embedding) == 0):\n",
    "            continue\n",
    "\n",
    "        embedding = np.array(embedding, dtype=np.float32).tolist()\n",
    "        point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"icp:{icp_id}\"))\n",
    "\n",
    "        point = PointStruct(\n",
    "            id=point_id,\n",
    "            vector=embedding,\n",
    "            payload={\n",
    "                \"icp_id\": icp_id,\n",
    "                \"industry\": industry,\n",
    "                \"location\": locations,\n",
    "                \"tech_stack\": tech_stack\n",
    "            }\n",
    "        )\n",
    "        icp_points.append(point)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding ICP {icp_id}: {e}\")\n",
    "\n",
    "# === Upsert to Qdrant ===\n",
    "if icp_points:\n",
    "    qdrant.upsert(collection_name=\"icp_vectors\", points=icp_points)\n",
    "    print(f\"Upserted {len(icp_points)} ICP profiles to Qdrant.\")\n",
    "else:\n",
    "    print(\"No valid ICP vectors to upsert.\")\n",
    "    print(f\"[{icp_id}] vector_text: '{vector_text}' | filters: {filters}\")\n",
    "    print(f\"[{icp_id}] Embedding generated: {embedding is not None} | Zero vector: {np.all(np.array(embedding)==0) if embedding is not None else 'N/A'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d7315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13106821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fintech', 'finance']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14884a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fintech', 'finance', 'ai', 'aws', 'python', 'langchain']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d671d03e",
   "metadata": {},
   "source": [
    "#### cos_sim scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbaacac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total points returned from Qdrant: 5\n",
      "Scoring complete using util.cos_sim(). Top results:\n",
      "ssynthai.io  Score: 3.1059\n",
      "flowcast.ai  Score: 2.2858\n",
      "neogencloud.com  Score: 0.0691\n",
      "retico.ai  Score: 0.0085\n",
      "ddocstream.com  Score: -0.6692\n"
     ]
    }
   ],
   "source": [
    "# WEIGHTS = {\n",
    "#     \"industry\": 20,\n",
    "#     \"employee_count\": 15,\n",
    "#     \"location\": 15,\n",
    "#     \"tech_stack\": 25,\n",
    "#     \"keywords\": 10,\n",
    "#     \"founded_year\": 5,\n",
    "#     \"github_signal\": 10\n",
    "# }\n",
    " \n",
    "# # --- 1. Get latest active ICP ---\n",
    "# latest_icp = list(icp_profiles.aggregate(pipeline))\n",
    " \n",
    "# if not latest_icp:\n",
    "#     print(\" No active ICP found.\")\n",
    "#     exit()\n",
    "# icp = latest_icp[0]\n",
    "# icp_id = str(icp[\"_id\"])\n",
    "# icp_version = icp.get(\"version\", 1)\n",
    " \n",
    "# # --- 2. Get ICP vector from Qdrant ---\n",
    " \n",
    "# icp_vector_resp = qdrant.retrieve(\n",
    "#     collection_name=\"icp_vectors\",\n",
    "#     ids= [str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"icp:{icp_id}\"))],\n",
    "#     with_vectors=True\n",
    "# )\n",
    " \n",
    "# if not icp_vector_resp or not icp_vector_resp[0].vector:\n",
    "#     print(\" ICP vector not found.\")\n",
    "#     exit()\n",
    " \n",
    "# icp_embedding = icp_vector_resp[0].vector\n",
    " \n",
    "# # --- 3. Get all company vectors from Qdrant ---\n",
    "# search_result = qdrant.query_points(\n",
    "#     collection_name=\"company_vectors\",\n",
    "#     query=icp_embedding,\n",
    "#     limit=10000,\n",
    "#     with_payload=True,\n",
    "#     with_vectors=True\n",
    "# )\n",
    "# print(f\"Total points returned from Qdrant: {len(search_result.points)}\")\n",
    " \n",
    "# icp_founded_after = icp.get(\"filters\", {}).get(\"founded_after\", 2015)\n",
    " \n",
    "# # --- 4. Scoring Rule Function () ---\n",
    "# def rule_score(company_doc, icp_tokens, icp_founded_after):\n",
    "#     rule_score = 0\n",
    "#     breakdown = {k: 0 for k in WEIGHTS}\n",
    "#     def safe_list(field):\n",
    "#         value = company_doc.get(field, [])\n",
    "#         if isinstance(value, str): return [value]\n",
    "#         if isinstance(value, list): return value\n",
    "#         return []\n",
    "    \n",
    "#     # Match tokens in industry, location, tech stack, keywords\n",
    "#     for field, key in [(\"industries\", \"industry\"), (\"hq_location\", \"location\"),\n",
    "#                        (\"tech_stack\", \"tech_stack\"), (\"keywords\", \"keywords\")]:\n",
    "#         values = (\n",
    "#             safe_list(field) if field != \"hq_location\"\n",
    "#             else list(company_doc.get(\"hq_location\", {}).values())\n",
    "#         )\n",
    "#         if any(token in str(v).lower() for v in values for token in icp_tokens):\n",
    "#             rule_score += WEIGHTS[key]\n",
    "#             breakdown[key] = WEIGHTS[key]\n",
    " \n",
    "#     # Match employee count\n",
    "#     emp = company_doc.get(\"employee_count_estimate\", {})\n",
    "#     if emp.get(\"min\", 0) >= 10 and emp.get(\"max\", 0) <= 500:\n",
    "#         rule_score += WEIGHTS[\"employee_count\"]\n",
    "#         breakdown[\"employee_count\"] = WEIGHTS[\"employee_count\"]\n",
    " \n",
    "#     # Founded year match\n",
    "#     if isinstance(company_doc.get(\"founded_year\"), int) and company_doc[\"founded_year\"] >= icp_founded_after:\n",
    "#         rule_score += WEIGHTS[\"founded_year\"]\n",
    "#         breakdown[\"founded_year\"] = WEIGHTS[\"founded_year\"]\n",
    " \n",
    "#     # GitHub presence\n",
    "#     urls = company_doc.get(\"source_urls\", [])\n",
    "#     if any(\"github\" in url.lower() for url in urls):\n",
    "#         rule_score += WEIGHTS[\"github_signal\"]\n",
    "#         breakdown[\"github_signal\"] = WEIGHTS[\"github_signal\"]\n",
    " \n",
    "#     return rule_score, breakdown\n",
    " \n",
    "# # --- 5. Tokenize ICP filters ---\n",
    "# icp_tokens = [token.lower().strip() for token in f\"{' '.join(icp.get('filters', {}).get('industries', []))} {' '.join(icp.get('filters', {}).get('locations', []))}\".split()]\n",
    " \n",
    "# # --- 6. Score companies using util.cos_sim() ---\n",
    "# scored_results_util = []\n",
    "# icp_tensor = torch.tensor(model.encode([' '.join(icp_tokens)]), dtype=torch.float32)  # Convert ICP to tensor\n",
    " \n",
    "# for point in search_result.points:\n",
    "#     payload = point.payload\n",
    "#     company_id = payload.get(\"company_id\")\n",
    "#     company_vector = point.vector\n",
    "    \n",
    "#     # Calculate cosine similarity using util.cos_sim()\n",
    "#     company_tensor = torch.tensor([company_vector], dtype=torch.float32)\n",
    "#     similarity = util.cos_sim(icp_tensor, company_tensor)[0][0].item()\n",
    "    \n",
    "#     try:\n",
    "#         if isinstance(company_id, str):\n",
    "#             company_id = ObjectId(company_id)\n",
    "#     except Exception as e:\n",
    "#         print(f'{company_id} have {e}')\n",
    "#         continue \n",
    " \n",
    "#     company = discovered_companies.find_one({\"_id\": company_id})\n",
    "#     if not company:\n",
    "#         continue\n",
    "        \n",
    "#     rule, breakdown = rule_score(company, icp_tokens, icp_founded_after)\n",
    "#     if rule == 0:\n",
    "#         final_score = round(similarity * 0.1, 4)\n",
    "#     else:\n",
    "#         final_score = round(similarity * rule, 4)\n",
    "    \n",
    "#     breakdown[\"vector_similarity\"] = round(similarity, 4)\n",
    " \n",
    "#     scored_doc = {\n",
    "#         \"company_id\": company_id,\n",
    "#         \"icp_id\": icp_id,\n",
    "#         \"icp_version\": icp_version,\n",
    "#         \"final_score\": final_score,\n",
    "#         \"breakdown\": breakdown,\n",
    "#         \"weights\": WEIGHTS,\n",
    "#         \"last_scored\": datetime.now(timezone.utc),\n",
    "#         \"method\": \"util_cos_sim\"\n",
    "#     }\n",
    "    \n",
    "#     # Store in a different collection or add method flag\n",
    "#     scored_companies.update_one(\n",
    "#         {\"company_id\": company_id, \"icp_id\": icp_id, \"method\": \"util_cos_sim\"},\n",
    "#         {\"$set\": scored_doc},\n",
    "#         upsert=True\n",
    "#     )\n",
    "#     scored_results_util.append((company.get(\"domain\", \"unknown\"), final_score))\n",
    " \n",
    "# # --- 7. Print Top Results ---\n",
    "# print(\"Scoring complete using util.cos_sim(). Top results:\")\n",
    "# for domain, score in sorted(scored_results_util, key=lambda x: x[1], reverse=True)[:20]:\n",
    "#     print(f\"{domain}  Score: {score}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222c51f",
   "metadata": {},
   "source": [
    "## Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d2bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total points returned from Qdrant: 5\n",
      " Extracted ICP tokens: ['fintech', 'aws', 'ai', 'langchain', 'gen', 'ml', 'python', 'finance']\n",
      " Total companies scored: 5\n",
      "\n",
      " TOP RESULTS (Weighted Average Method):\n",
      " 1. flowcast.ai            76.58%\n",
      " 2. ssynthai.io            69.80%\n",
      " 3. neogencloud.com        61.05%\n",
      " 4. retico.ai              58.12%\n",
      " 5. ddocstream.com         49.36%\n"
     ]
    }
   ],
   "source": [
    "WEIGHTS = {\n",
    "    \"industry\": 20,\n",
    "    \"employee_count\": 15,\n",
    "    \"location\": 15,\n",
    "    \"tech_stack\": 25,\n",
    "    \"keywords\": 10,\n",
    "    \"founded_year\": 5,\n",
    "    \"github_signal\": 10\n",
    "}\n",
    " \n",
    "# --- 1. Get latest active ICP ---\n",
    "latest_icp = list(icp_profiles.aggregate(pipeline))\n",
    " \n",
    "if not latest_icp:\n",
    "    print(\" No active ICP found.\")\n",
    "    exit()\n",
    "icp = latest_icp[0]\n",
    "icp_id = str(icp[\"_id\"])\n",
    "icp_version = icp.get(\"version\", 1)\n",
    " \n",
    "# --- 2. Get ICP vector from Qdrant ---\n",
    " \n",
    "icp_vector_resp = qdrant.retrieve(\n",
    "    collection_name=\"icp_vectors\",\n",
    "    ids= [str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"icp:{icp_id}\"))],\n",
    "    with_vectors=True\n",
    ")\n",
    " \n",
    "if not icp_vector_resp or not icp_vector_resp[0].vector:\n",
    "    print(\" ICP vector not found.\")\n",
    "    exit()\n",
    " \n",
    "icp_embedding = icp_vector_resp[0].vector\n",
    " \n",
    "# --- 3. Get all company vectors from Qdrant ---\n",
    "search_result = qdrant.query_points(\n",
    "    collection_name=\"company_vectors\",\n",
    "    query=icp_embedding,\n",
    "    limit=10000,\n",
    "    with_payload=True,\n",
    "    with_vectors=True\n",
    ")\n",
    "print(f\"Total points returned from Qdrant: {len(search_result.points)}\")\n",
    " \n",
    "icp_founded_after = icp.get(\"filters\", {}).get(\"founded_after\", 2015)\n",
    " \n",
    "# --- 4. Scoring Rule Function () ---\n",
    "def rule_score(company_doc, icp_tokens, icp_founded_after):\n",
    "    rule_score = 0\n",
    "    breakdown = {k: 0 for k in WEIGHTS}\n",
    "    def safe_list(field):\n",
    "        value = company_doc.get(field, [])\n",
    "        if isinstance(value, str): return [value]\n",
    "        if isinstance(value, list): return value\n",
    "        return []\n",
    "    \n",
    "    # Match tokens in industry, location, tech stack, keywords\n",
    "    for field, key in [(\"industries\", \"industry\"), (\"hq_location\", \"location\"),\n",
    "                       (\"tech_stack\", \"tech_stack\"), (\"keywords\", \"keywords\")]:\n",
    "        values = (\n",
    "            safe_list(field) if field != \"hq_location\"\n",
    "            else list(company_doc.get(\"hq_location\", {}).values())\n",
    "        )\n",
    "        if any(token in str(v).lower() for v in values for token in icp_tokens):\n",
    "            rule_score += WEIGHTS[key]\n",
    "            breakdown[key] = WEIGHTS[key]\n",
    " \n",
    "    # Match employee count\n",
    "    emp = company_doc.get(\"employee_count_estimate\", {})\n",
    "    if emp.get(\"min\", 0) >= 10 and emp.get(\"max\", 0) <= 500:\n",
    "        rule_score += WEIGHTS[\"employee_count\"]\n",
    "        breakdown[\"employee_count\"] = WEIGHTS[\"employee_count\"]\n",
    " \n",
    "    # Founded year match\n",
    "    if isinstance(company_doc.get(\"founded_year\"), int) and company_doc[\"founded_year\"] >= icp_founded_after:\n",
    "        rule_score += WEIGHTS[\"founded_year\"]\n",
    "        breakdown[\"founded_year\"] = WEIGHTS[\"founded_year\"]\n",
    " \n",
    "    # GitHub presence\n",
    "    urls = company_doc.get(\"source_urls\", [])\n",
    "    if any(\"github\" in url.lower() for url in urls):\n",
    "        rule_score += WEIGHTS[\"github_signal\"]\n",
    "        breakdown[\"github_signal\"] = WEIGHTS[\"github_signal\"]\n",
    " \n",
    "    return rule_score, breakdown\n",
    " \n",
    "# --- 5. Tokenize ICP filters ---\n",
    "icp_filters = icp.get('filters', {})\n",
    "icp_tokens =[]\n",
    "\n",
    "for field_name in ['industry', 'industries']:\n",
    "    if field_name in icp_filters:\n",
    "        industries = icp_filters[field_name]\n",
    "        if isinstance(industries, list):\n",
    "            for industry in industries:\n",
    "                tokens = str(industry).lower().split()\n",
    "                icp_tokens.extend(tokens)\n",
    "        elif isinstance(industries, str):\n",
    "            tokens = industries.lower().split()\n",
    "            icp_tokens.extend(tokens)\n",
    "\n",
    "# Extract tech stack tokens\n",
    "if 'tech_stack' in icp_filters:\n",
    "    tech_stack = icp_filters['tech_stack']\n",
    "    if isinstance(tech_stack, list):\n",
    "        for tech in tech_stack:\n",
    "            # Keep full tech names for exact matching\n",
    "            icp_tokens.append(str(tech).lower())\n",
    "    elif isinstance(tech_stack, str):\n",
    "        icp_tokens.append(tech_stack.lower())\n",
    " \n",
    "# Extract keyword tokens\n",
    "if 'keywords' in icp_filters:\n",
    "    keywords = icp_filters['keywords']\n",
    "    if isinstance(keywords, list):\n",
    "        for keyword in keywords:\n",
    "            icp_tokens.append(str(keyword).lower())\n",
    "    elif isinstance(keywords, str):\n",
    "        icp_tokens.append(keywords.lower())\n",
    " \n",
    "# Clean up tokens\n",
    "icp_tokens = [token.strip() for token in icp_tokens if token.strip()]\n",
    "icp_tokens = list(set(icp_tokens))  # Remove duplicates\n",
    " \n",
    "print(f\" Extracted ICP tokens: {icp_tokens}\")\n",
    " \n",
    "\n",
    "icp_tokens = list(set([token for token in icp_tokens if token.strip()]))\n",
    "\n",
    "# --- 6. Score companies using util.cos_sim() ---\n",
    "scored_results_util = []\n",
    "icp_tensor = torch.tensor([icp_embedding], dtype=torch.float32)  # Convert ICP to tensor\n",
    "\n",
    "for point in search_result.points:\n",
    "    payload = point.payload\n",
    "    company_id = payload.get(\"company_id\")\n",
    "    company_vector = point.vector\n",
    "\n",
    "    # Calculate cosine similarity using util.cos_sim()\n",
    "    company_tensor = torch.tensor([company_vector], dtype=torch.float32)\n",
    "    raw_similarity = util.cos_sim(icp_tensor, company_tensor)[0][0].item()\n",
    "    vector_similarity_score = ((raw_similarity + 1) / 2) * 100  # Normalize to 0100\n",
    "\n",
    "    try:\n",
    "        if isinstance(company_id, str):\n",
    "            company_id = ObjectId(company_id)\n",
    "    except Exception as e:\n",
    "        print(f'{company_id} has invalid ObjectId: {e}')\n",
    "        continue \n",
    "\n",
    "    company = discovered_companies.find_one({\"_id\": company_id})\n",
    "    if not company:\n",
    "        continue\n",
    "\n",
    "    # Apply rule-based scoring\n",
    "    rule_score_total, breakdown = rule_score(company, icp_tokens, icp_founded_after)\n",
    "\n",
    "    # Final score weights\n",
    "    vector_weight = 0.4\n",
    "    rule_weight = 0.6\n",
    "\n",
    "    final_score = round(\n",
    "        (vector_similarity_score * vector_weight) + (rule_score_total * rule_weight),\n",
    "        2\n",
    "    )\n",
    "\n",
    "    breakdown[\"vector_similarity\"] = round(vector_similarity_score, 2)\n",
    "\n",
    "    scored_doc = {\n",
    "        \"company_id\": company_id,\n",
    "        \"icp_id\": icp_id,\n",
    "        \"icp_version\": icp_version,\n",
    "        \"final_score\": final_score,\n",
    "        \"breakdown\": breakdown,\n",
    "        \"weights\": WEIGHTS,\n",
    "        \"last_scored\": datetime.now(timezone.utc),\n",
    "        \"method\": \"util_cos_sim\"\n",
    "    }\n",
    "\n",
    "    scored_companies.update_one(\n",
    "        {\"company_id\": company_id, \"icp_id\": icp_id, \"method\": \"util_cos_sim\"},\n",
    "        {\"$set\": scored_doc},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "    scored_results_util.append((company.get(\"domain\", \"unknown\"), final_score))\n",
    "\n",
    "# --- 7. Print Top Results ---\n",
    "print(f\" Total companies scored: {len(scored_results_util)}\")\n",
    "\n",
    "sorted_results = sorted(scored_results_util, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n TOP RESULTS (Weighted Average Method):\")\n",
    "for i, (domain, score) in enumerate(sorted_results[:10], 1):\n",
    "    print(f\"{i:2d}. {domain:20s}  {score:6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1aaab",
   "metadata": {},
   "source": [
    "## test code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293b3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total points returned from Qdrant: 5\n",
      " Extracted ICP tokens: ['fintech', 'finance']\n",
      " Field-specific tokens: {'industry': ['fintech', 'finance'], 'location': [], 'employee_count': {'min': 30, 'max': 20}, 'founded_year': 2019}\n",
      " Total companies scored: 5\n",
      "\n",
      " TOP RESULTS (Weighted Average Method):\n",
      " 1. flowcast.ai            52.58%\n",
      " 2. neogencloud.com        43.05%\n",
      " 3. ssynthai.io            42.80%\n",
      " 4. retico.ai              31.12%\n",
      " 5. ddocstream.com         25.36%\n"
     ]
    }
   ],
   "source": [
    "WEIGHTS = {\n",
    "    \"industry\": 20,\n",
    "    \"employee_count\": 15,\n",
    "    \"location\": 15,\n",
    "    \"tech_stack\": 25,\n",
    "    \"keywords\": 10,\n",
    "    \"founded_year\": 5,\n",
    "    \"github_signal\": 10\n",
    "}\n",
    " \n",
    "# --- 1. Get latest active ICP ---\n",
    "latest_icp = list(icp_profiles.aggregate(pipeline))\n",
    " \n",
    "if not latest_icp:\n",
    "    print(\" No active ICP found.\")\n",
    "    exit()\n",
    "icp = latest_icp[0]\n",
    "icp_id = str(icp[\"_id\"])\n",
    "icp_version = icp.get(\"version\", 1)\n",
    " \n",
    "# --- 2. Get ICP vector from Qdrant ---\n",
    " \n",
    "icp_vector_resp = qdrant.retrieve(\n",
    "    collection_name=\"icp_vectors\",\n",
    "    ids= [str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"icp:{icp_id}\"))],\n",
    "    with_vectors=True\n",
    ")\n",
    " \n",
    "if not icp_vector_resp or not icp_vector_resp[0].vector:\n",
    "    print(\" ICP vector not found.\")\n",
    "    exit()\n",
    " \n",
    "icp_embedding = icp_vector_resp[0].vector\n",
    " \n",
    "# --- 3. Get all company vectors from Qdrant ---\n",
    "search_result = qdrant.query_points(\n",
    "    collection_name=\"company_vectors\",\n",
    "    query=icp_embedding,\n",
    "    limit=10000,\n",
    "    with_payload=True,\n",
    "    with_vectors=True\n",
    ")\n",
    "print(f\"Total points returned from Qdrant: {len(search_result.points)}\")\n",
    " \n",
    "icp_founded_after = icp.get(\"filters\", {}).get(\"founded_after\", 2015)\n",
    " \n",
    "# --- 4. Scoring Rule Function () ---\n",
    "def rule_score(company_doc, icp_tokens, icp_founded_after, icp_field_tokens=None):\n",
    "    rule_score = 0\n",
    "    breakdown = {k: 0 for k in WEIGHTS}\n",
    "    def safe_list(field):\n",
    "        value = company_doc.get(field, [])\n",
    "        if isinstance(value, str): return [value]\n",
    "        if isinstance(value, list): return value\n",
    "        return []\n",
    "    \n",
    "    # 1. INDUSTRY MATCHING\n",
    "    if icp_field_tokens and 'industry' in icp_field_tokens:\n",
    "        industry_tokens = icp_field_tokens['industry']\n",
    "        company_industries = safe_list(\"industries\")\n",
    "        if any(token in str(v).lower() for v in company_industries for token in industry_tokens):\n",
    "            rule_score += WEIGHTS[\"industry\"]\n",
    "            breakdown[\"industry\"] = WEIGHTS[\"industry\"]\n",
    "    \n",
    "    # 2. LOCATION MATCHING  \n",
    "    if icp_field_tokens and 'location' in icp_field_tokens:\n",
    "        location_tokens = icp_field_tokens['location']\n",
    "        company_location_values = list(company_doc.get(\"hq_location\", {}).values())\n",
    "        if any(token in str(v).lower() for v in company_location_values for token in location_tokens):\n",
    "            rule_score += WEIGHTS[\"location\"]\n",
    "            breakdown[\"location\"] = WEIGHTS[\"location\"]\n",
    "    \n",
    "    # 3. TECH STACK MATCHING\n",
    "    if icp_field_tokens and 'tech_stack' in icp_field_tokens:\n",
    "        tech_tokens = icp_field_tokens['tech_stack']\n",
    "        company_tech = safe_list(\"tech_stack\")\n",
    "        if any(token in str(v).lower() for v in company_tech for token in tech_tokens):\n",
    "            rule_score += WEIGHTS[\"tech_stack\"]\n",
    "            breakdown[\"tech_stack\"] = WEIGHTS[\"tech_stack\"]\n",
    "    \n",
    "    # 4. KEYWORDS MATCHING\n",
    "    if icp_field_tokens and 'keywords' in icp_field_tokens:\n",
    "        keyword_tokens = icp_field_tokens['keywords']\n",
    "        text_fields = [\n",
    "            company_doc.get(\"description\", \"\"),\n",
    "            company_doc.get(\"name\", \"\"),\n",
    "            \" \".join(safe_list(\"industries\")),\n",
    "            \" \".join(safe_list(\"tech_stack\"))\n",
    "        ]\n",
    "        company_text = \" \".join(text_fields).lower()\n",
    "        if any(token in company_text for token in keyword_tokens):\n",
    "            rule_score += WEIGHTS[\"keywords\"]\n",
    "            breakdown[\"keywords\"] = WEIGHTS[\"keywords\"]\n",
    "    # 5. EMPLOYEE COUNT MATCHING (DYNAMIC)\n",
    "    emp = company_doc.get(\"employee_count_estimate\", {})\n",
    "    emp_min = emp.get(\"min\", 0)\n",
    "    emp_max = emp.get(\"max\", float('inf'))\n",
    "    \n",
    "    if icp_field_tokens and 'employee_count' in icp_field_tokens:\n",
    "        icp_emp = icp_field_tokens['employee_count']\n",
    "        if isinstance(icp_emp, dict):\n",
    "            icp_min = icp_emp.get('min')\n",
    "            icp_max = icp_emp.get('max')\n",
    "            \n",
    "            # Check if company employee count falls within the ICP range\n",
    "            if not (emp_max< icp_min or emp_min > icp_max):\n",
    "                rule_score += WEIGHTS[\"employee_count\"]\n",
    "                breakdown[\"employee_count\"] = WEIGHTS[\"employee_count\"]\n",
    "    # 6. FOUNDED YEAR MATCHING (DYNAMIC)\n",
    "    founded_year = company_doc.get(\"founded_year\")\n",
    "    \n",
    "    if icp_field_tokens and 'founded_year' in icp_field_tokens:\n",
    "        year_threshold = icp_field_tokens['founded_year']\n",
    "        \n",
    "        if isinstance(founded_year, int) and founded_year >= year_threshold:\n",
    "            rule_score += WEIGHTS[\"founded_year\"]\n",
    "            breakdown[\"founded_year\"] = WEIGHTS[\"founded_year\"]\n",
    "    # 7. GITHUB SIGNAL MATCHING\n",
    "    urls = company_doc.get(\"source_urls\", [])\n",
    "    if any(\"github\" in url.lower() for url in urls):\n",
    "        rule_score += WEIGHTS[\"github_signal\"]\n",
    "        breakdown[\"github_signal\"] = WEIGHTS[\"github_signal\"]\n",
    "    return rule_score, breakdown\n",
    " \n",
    "\n",
    "# --- 5. Tokenize ICP filters ---\n",
    "icp_filters = icp.get('filters', {})\n",
    "icp_tokens = []\n",
    "icp_field_tokens = {}  # Track tokens by field for better debugging\n",
    " \n",
    "# Extract industry tokens\n",
    "for field_name in ['industry', 'industries']:\n",
    "    if field_name in icp_filters:\n",
    "        industries = icp_filters[field_name]\n",
    "        field_tokens = []\n",
    "        if isinstance(industries, list):\n",
    "            for industry in industries:\n",
    "                tokens = str(industry).lower().split()\n",
    "                field_tokens.extend(tokens)\n",
    "                icp_tokens.extend(tokens)\n",
    "        elif isinstance(industries, str):\n",
    "            tokens = industries.lower().split()\n",
    "            field_tokens.extend(tokens)\n",
    "            icp_tokens.extend(tokens)\n",
    "        icp_field_tokens['industry'] = field_tokens\n",
    " \n",
    "# Extract location tokens\n",
    "for field_name in ['location', 'locations']:\n",
    "    if field_name in icp_filters:\n",
    "        locations = icp_filters[field_name]\n",
    "        field_tokens = []\n",
    "        if isinstance(locations, list):\n",
    "            for location in locations:\n",
    "                tokens = str(location).lower().split()\n",
    "                field_tokens.extend(tokens)\n",
    "                icp_tokens.extend(tokens)\n",
    "        elif isinstance(locations, str):\n",
    "            tokens = locations.lower().split()\n",
    "            field_tokens.extend(tokens)\n",
    "            icp_tokens.extend(tokens)\n",
    "        icp_field_tokens['location'] = field_tokens\n",
    " \n",
    "# Extract tech stack tokens\n",
    "if 'tech_stack' in icp_filters:\\n",
    "    tech_stack = icp_filters['tech_stack']\\n",
    "    field_tokens = []\n",
    "    if isinstance(tech_stack, list):\n",
    "        for tech in tech_stack:\n",
    "            # Keep full tech names for exact matching\n",
    "            token = str(tech).lower()\n",
    "            field_tokens.append(token)\n",
    "            icp_tokens.append(token)\n",
    "    elif isinstance(tech_stack, str):\n",
    "        token = tech_stack.lower()\n",
    "        field_tokens.append(token)\n",
    "        icp_tokens.append(token)\n",
    "    icp_field_tokens['tech_stack'] = field_tokens\n",
    " \n",
    "# Extract keyword tokens\n",
    "if 'keywords' in icp_filters:\\n",
    "    keywords = icp_filters['keywords']\\n",
    "    field_tokens = []\n",
    "    if isinstance(keywords, list):\n",
    "        for keyword in keywords:\n",
    "            token = str(keyword).lower()\n",
    "            field_tokens.append(token)\n",
    "            icp_tokens.append(token)\n",
    "    elif isinstance(keywords, str):\n",
    "        token = keywords.lower()\n",
    "        field_tokens.append(token)\n",
    "        icp_tokens.append(token)\n",
    "    icp_field_tokens['keywords'] = field_tokens\n",
    " \n",
    "# Extract employee count range (for matching logic)\n",
    "if 'employee_count' in icp_filters:\n",
    "    emp_filter = icp_filters['employee_count']\n",
    "    icp_field_tokens['employee_count'] = emp_filter  # Store the range dict\n",
    " \n",
    "# Extract founded year (for matching logic)\n",
    "if 'founded_after' in icp_filters:\\n",
    "    icp_field_tokens['founded_year'] = icp_filters['founded_after']\\n",
    "elif 'founded_after' in icp:\\n",
    "    icp_field_tokens['founded_year'] = icp['founded_after']\n",
    " \n",
    "# Clean up tokens\n",
    "icp_tokens = [token.strip() for token in icp_tokens if token.strip()]\n",
    "icp_tokens = list(set(icp_tokens))  # Remove duplicates\n",
    " \n",
    "print(f\" Extracted ICP tokens: {icp_tokens}\")\n",
    "print(f\" Field-specific tokens: {icp_field_tokens}\")\n",
    " \n",
    " \n",
    "\n",
    "icp_tokens = list(set([token for token in icp_tokens if token.strip()]))\n",
    "\n",
    "# --- 6. Score companies using util.cos_sim() ---\n",
    "scored_results_util = []\n",
    "icp_tensor = torch.tensor([icp_embedding], dtype=torch.float32)  # Convert ICP to tensor\n",
    "\n",
    "for point in search_result.points:\n",
    "    payload = point.payload\n",
    "    company_id = payload.get(\"company_id\")\n",
    "    company_vector = point.vector\n",
    "\n",
    "    # Calculate cosine similarity using util.cos_sim()\n",
    "    company_tensor = torch.tensor([company_vector], dtype=torch.float32)\n",
    "    raw_similarity = util.cos_sim(icp_tensor, company_tensor)[0][0].item()\n",
    "    vector_similarity_score = ((raw_similarity + 1) / 2) * 100  # Normalize to 0100\n",
    "\n",
    "    try:\n",
    "        if isinstance(company_id, str):\n",
    "            company_id = ObjectId(company_id)\n",
    "    except Exception as e:\n",
    "        print(f'{company_id} has invalid ObjectId: {e}')\n",
    "        continue \n",
    "\n",
    "    company = discovered_companies.find_one({\"_id\": company_id})\n",
    "    if not company:\n",
    "        continue\n",
    "\n",
    "    # Apply rule-based scoring\n",
    "    rule_score_total, breakdown = rule_score(company, icp_tokens, icp_founded_after, icp_field_tokens)\n",
    "\n",
    "    # Final score weights\n",
    "    vector_weight = 0.4\n",
    "    rule_weight = 0.6\n",
    "\n",
    "    final_score = round(\n",
    "        (vector_similarity_score * vector_weight) + (rule_score_total * rule_weight),\n",
    "        2\n",
    "    )\n",
    "\n",
    "    breakdown[\"vector_similarity\"] = round(vector_similarity_score, 2)\n",
    "\n",
    "    scored_doc = {\n",
    "        \"company_id\": company_id,\n",
    "        \"icp_id\": icp_id,\n",
    "        \"icp_version\": icp_version,\n",
    "        \"final_score\": final_score,\n",
    "        \"breakdown\": breakdown,\n",
    "        \"weights\": WEIGHTS,\n",
    "        \"last_scored\": datetime.now(timezone.utc),\n",
    "        \"method\": \"util_cos_sim\"\n",
    "    }\n",
    "\n",
    "    scored_companies.update_one(\n",
    "        {\"company_id\": company_id, \"icp_id\": icp_id, \"method\": \"util_cos_sim\"},\n",
    "        {\"$set\": scored_doc},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "    scored_results_util.append((company.get(\"domain\", \"unknown\"), final_score))\n",
    "\n",
    "# --- 7. Print Top Results ---\n",
    "print(f\" Total companies scored: {len(scored_results_util)}\")\n",
    "\n",
    "sorted_results = sorted(scored_results_util, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n TOP RESULTS (Weighted Average Method):\")\n",
    "for i, (domain, score) in enumerate(sorted_results[:10], 1):\n",
    "    print(f\"{i:2d}. {domain:20s}  {score:6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88239f47",
   "metadata": {},
   "source": [
    "#### Store to MongoDB :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914773d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scored_companies = db['scored_companies']\n",
    "\n",
    "# Call this after scoring each company\n",
    "def store_scored_company(company, score_tuple, icp_id, icp_version=1):\n",
    "    score, breakdown, similarity = score_tuple\n",
    "\n",
    "    company_oid = ObjectId(company[\"_id\"])\n",
    "    icp_oid = ObjectId(icp_id)\n",
    "\n",
    "    doc = {\n",
    "        \"company_id\": company_oid,\n",
    "        \"icp_id\": icp_oid,\n",
    "        \"score\": float(score),\n",
    "        \"breakdown\": {\n",
    "            **breakdown,\n",
    "            \"vector_similarity\": float(similarity)\n",
    "        },\n",
    "        \"last_scored\": datetime.now(timezone.utc),\n",
    "        \"icp_version\": icp_version\n",
    "    }\n",
    "\n",
    "    #  Upsert safely by matching ObjectId fields\n",
    "    scored_companies.update_one(\n",
    "        {\"company_id\": company_oid, \"icp_id\": icp_oid},\n",
    "        {\"$set\": doc},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "# Store in a different collection or add method flag\n",
    "    scored_companies.update_one(\n",
    "        {\"company_id\": company_id, \"icp_id\": icp_id, \"method\": \"util_cos_sim\"},\n",
    "        {\"$set\": scored_doc},\n",
    "        upsert=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c31277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation (similarity vs. final score): 0.917\n"
     ]
    }
   ],
   "source": [
    "final_scores = []\n",
    "similarities = []\n",
    "\n",
    "for doc in scored_companies.find():\n",
    "    similarities.append(doc[\"breakdown\"][\"vector_similarity\"])\n",
    "    final_scores.append(doc[\"final_score\"])\n",
    "\n",
    "corr, _ = pearsonr(similarities, final_scores)\n",
    "print(f\"Pearson correlation (similarity vs. final score): {round(corr, 3)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation (similarity vs. final score): 0.935\n"
     ]
    }
   ],
   "source": [
    "final_scores = []\n",
    "similarities = []\n",
    "\n",
    "for doc in scored_companies.find():\n",
    "    similarities.append(doc[\"breakdown\"][\"vector_similarity\"])\n",
    "    final_scores.append(doc[\"final_score\"])\n",
    "    \n",
    "corr, _ = spearmanr(similarities, final_scores)\n",
    "print(f\"Spearman correlation (similarity vs. final score): {round(corr, 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
